{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f8927b5",
   "metadata": {},
   "source": [
    "# 목표 : spark DF DeepFM Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f17745",
   "metadata": {},
   "source": [
    "### 1. Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a7de25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-05 19:52:22.170355: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from modules.train import get_data\n",
    "from modules.DeepFM import DeepFM\n",
    "import modules.config as config\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, AUC\n",
    "import pandas as pd\n",
    "# data load\n",
    "test = pd.read_parquet('data/test.parquet')\n",
    "y_test = test['target']\n",
    "x_test = test.drop('target',axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65576219",
   "metadata": {},
   "source": [
    "### 2. model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65329675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Prepared...\n",
      "X shape: (32561, 108)\n",
      "# of Feature: 108\n",
      "# of Field: 14\n",
      "train/test save complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-05 19:52:28.391829: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# model load\n",
    "_, _, field_dict, field_index = get_data()\n",
    "fm = DeepFM(embedding_size=config.EMBEDDING_SIZE, num_feature=len(field_index),\n",
    "               num_field=len(field_dict), field_index=field_index)\n",
    "\n",
    "fm.build(input_shape = (1,len(field_index)))\n",
    "fm.load_weights('./weights/weights-epoch(10)-batch(256)-embedding(5).h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b8ddb",
   "metadata": {},
   "source": [
    "### 2. Pandas DF -> Spark DF convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3435070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: double (nullable = true)\n",
      " |-- workclass- ?: long (nullable = true)\n",
      " |-- workclass- Federal-gov: long (nullable = true)\n",
      " |-- workclass- Local-gov: long (nullable = true)\n",
      " |-- workclass- Never-worked: long (nullable = true)\n",
      " |-- workclass- Private: long (nullable = true)\n",
      " |-- workclass- Self-emp-inc: long (nullable = true)\n",
      " |-- workclass- Self-emp-not-inc: long (nullable = true)\n",
      " |-- workclass- State-gov: long (nullable = true)\n",
      " |-- workclass- Without-pay: long (nullable = true)\n",
      " |-- fnlwgt: double (nullable = true)\n",
      " |-- education- 10th: long (nullable = true)\n",
      " |-- education- 11th: long (nullable = true)\n",
      " |-- education- 12th: long (nullable = true)\n",
      " |-- education- 1st-4th: long (nullable = true)\n",
      " |-- education- 5th-6th: long (nullable = true)\n",
      " |-- education- 7th-8th: long (nullable = true)\n",
      " |-- education- 9th: long (nullable = true)\n",
      " |-- education- Assoc-acdm: long (nullable = true)\n",
      " |-- education- Assoc-voc: long (nullable = true)\n",
      " |-- education- Bachelors: long (nullable = true)\n",
      " |-- education- Doctorate: long (nullable = true)\n",
      " |-- education- HS-grad: long (nullable = true)\n",
      " |-- education- Masters: long (nullable = true)\n",
      " |-- education- Preschool: long (nullable = true)\n",
      " |-- education- Prof-school: long (nullable = true)\n",
      " |-- education- Some-college: long (nullable = true)\n",
      " |-- education-num: double (nullable = true)\n",
      " |-- marital-status- Divorced: long (nullable = true)\n",
      " |-- marital-status- Married-AF-spouse: long (nullable = true)\n",
      " |-- marital-status- Married-civ-spouse: long (nullable = true)\n",
      " |-- marital-status- Married-spouse-absent: long (nullable = true)\n",
      " |-- marital-status- Never-married: long (nullable = true)\n",
      " |-- marital-status- Separated: long (nullable = true)\n",
      " |-- marital-status- Widowed: long (nullable = true)\n",
      " |-- occupation- ?: long (nullable = true)\n",
      " |-- occupation- Adm-clerical: long (nullable = true)\n",
      " |-- occupation- Armed-Forces: long (nullable = true)\n",
      " |-- occupation- Craft-repair: long (nullable = true)\n",
      " |-- occupation- Exec-managerial: long (nullable = true)\n",
      " |-- occupation- Farming-fishing: long (nullable = true)\n",
      " |-- occupation- Handlers-cleaners: long (nullable = true)\n",
      " |-- occupation- Machine-op-inspct: long (nullable = true)\n",
      " |-- occupation- Other-service: long (nullable = true)\n",
      " |-- occupation- Priv-house-serv: long (nullable = true)\n",
      " |-- occupation- Prof-specialty: long (nullable = true)\n",
      " |-- occupation- Protective-serv: long (nullable = true)\n",
      " |-- occupation- Sales: long (nullable = true)\n",
      " |-- occupation- Tech-support: long (nullable = true)\n",
      " |-- occupation- Transport-moving: long (nullable = true)\n",
      " |-- relationship- Husband: long (nullable = true)\n",
      " |-- relationship- Not-in-family: long (nullable = true)\n",
      " |-- relationship- Other-relative: long (nullable = true)\n",
      " |-- relationship- Own-child: long (nullable = true)\n",
      " |-- relationship- Unmarried: long (nullable = true)\n",
      " |-- relationship- Wife: long (nullable = true)\n",
      " |-- race- Amer-Indian-Eskimo: long (nullable = true)\n",
      " |-- race- Asian-Pac-Islander: long (nullable = true)\n",
      " |-- race- Black: long (nullable = true)\n",
      " |-- race- Other: long (nullable = true)\n",
      " |-- race- White: long (nullable = true)\n",
      " |-- sex- Female: long (nullable = true)\n",
      " |-- sex- Male: long (nullable = true)\n",
      " |-- capital-gain: double (nullable = true)\n",
      " |-- capital-loss: double (nullable = true)\n",
      " |-- hours-per-week: double (nullable = true)\n",
      " |-- country- ?: long (nullable = true)\n",
      " |-- country- Cambodia: long (nullable = true)\n",
      " |-- country- Canada: long (nullable = true)\n",
      " |-- country- China: long (nullable = true)\n",
      " |-- country- Columbia: long (nullable = true)\n",
      " |-- country- Cuba: long (nullable = true)\n",
      " |-- country- Dominican-Republic: long (nullable = true)\n",
      " |-- country- Ecuador: long (nullable = true)\n",
      " |-- country- El-Salvador: long (nullable = true)\n",
      " |-- country- England: long (nullable = true)\n",
      " |-- country- France: long (nullable = true)\n",
      " |-- country- Germany: long (nullable = true)\n",
      " |-- country- Greece: long (nullable = true)\n",
      " |-- country- Guatemala: long (nullable = true)\n",
      " |-- country- Haiti: long (nullable = true)\n",
      " |-- country- Holand-Netherlands: long (nullable = true)\n",
      " |-- country- Honduras: long (nullable = true)\n",
      " |-- country- Hong: long (nullable = true)\n",
      " |-- country- Hungary: long (nullable = true)\n",
      " |-- country- India: long (nullable = true)\n",
      " |-- country- Iran: long (nullable = true)\n",
      " |-- country- Ireland: long (nullable = true)\n",
      " |-- country- Italy: long (nullable = true)\n",
      " |-- country- Jamaica: long (nullable = true)\n",
      " |-- country- Japan: long (nullable = true)\n",
      " |-- country- Laos: long (nullable = true)\n",
      " |-- country- Mexico: long (nullable = true)\n",
      " |-- country- Nicaragua: long (nullable = true)\n",
      " |-- country- Outlying-US(Guam-USVI-etc): long (nullable = true)\n",
      " |-- country- Peru: long (nullable = true)\n",
      " |-- country- Philippines: long (nullable = true)\n",
      " |-- country- Poland: long (nullable = true)\n",
      " |-- country- Portugal: long (nullable = true)\n",
      " |-- country- Puerto-Rico: long (nullable = true)\n",
      " |-- country- Scotland: long (nullable = true)\n",
      " |-- country- South: long (nullable = true)\n",
      " |-- country- Taiwan: long (nullable = true)\n",
      " |-- country- Thailand: long (nullable = true)\n",
      " |-- country- Trinadad&Tobago: long (nullable = true)\n",
      " |-- country- United-States: long (nullable = true)\n",
      " |-- country- Vietnam: long (nullable = true)\n",
      " |-- country- Yugoslavia: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#Create PySpark SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()\n",
    "#Create PySpark DataFrame from Pandas\n",
    "sparkDF=spark.createDataFrame(x_test) \n",
    "sparkDF.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e035b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark.sql.functions as F\n",
    "# from pyspark.ml.feature import OneHotEncoder\n",
    "# from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# indexer = StringIndexer(inputCols=CAT_FIELDS, outputCols=[col + \"_encoded\" for col in CAT_FIELDS])\n",
    "# label_df = indexer.fit(sparkDF).transform(sparkDF)\n",
    "# label_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f1458cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0488150417804718"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = float(fm(x_test.head(1).values))\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55487bc",
   "metadata": {},
   "source": [
    "### UDF predit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "128cc5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# column order required by the model.\n",
    "FEATURES = x_test.columns\n",
    "path = './weights/weights-epoch(10)-batch(256)-embedding(5).h5' \n",
    "\n",
    "def predict(features):\n",
    "\n",
    "    np_features = np.array([features])\n",
    "    \n",
    "    \n",
    "#     # model load\n",
    "#     fm = DeepFM(embedding_size=config.EMBEDDING_SIZE, num_feature=len(field_index),\n",
    "#                num_field=len(field_dict), field_index=field_index)\n",
    "\n",
    "#     fm.build(input_shape = (1,len(field_index)))\n",
    "#     fm.load_weights(path)\n",
    "    \n",
    "    y = fm(np_features)\n",
    "\n",
    "\n",
    "    return float(y)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    " \n",
    "\n",
    "predict_udf = udf(predict, DoubleType())\n",
    "\n",
    "\n",
    "# test용 udf\n",
    "def test(x):\n",
    "    return x \n",
    "test_udf = udf(test, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8384f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparkDF2 = sparkDF.withColumn(\n",
    "#     \"test\",\n",
    "#     test_udf(F.col('country- Philippines'))\n",
    "# )\n",
    "# sparkDF2.select(F.col('test')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c060ea29",
   "metadata": {},
   "source": [
    "### MapPartitions predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aa1a5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# column order required by the model.\n",
    "FEATURES = x_test.columns\n",
    "path = './weights/weights-epoch(10)-batch(256)-embedding(5).h5' \n",
    "\n",
    "def predict_partition(rows):\n",
    "    # model load\n",
    "    fm = DeepFM(embedding_size=config.EMBEDDING_SIZE, num_feature=len(field_index),\n",
    "               num_field=len(field_dict), field_index=field_index)\n",
    "    fm.build(input_shape = (1,len(field_index)))\n",
    "    fm.load_weights(path)\n",
    "    \n",
    "    y = fm(rows[FEATURES].values)\n",
    "    return y\n",
    "\n",
    "#     y = fm(rows[FEATURES].values)\n",
    "#     return float(y)\n",
    "\n",
    "\n",
    "def test(rows):\n",
    "    for row in rows:\n",
    "        yield row.age + row['workclass- Federal-gov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ef1d8b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = df.rdd.mapPartitions(test).collect()\n",
    "# test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "52fe9cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04881441965699196"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(fm(x_test.head(1).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6f73dbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[age: double, workclass- ?: smallint, workclass- Federal-gov: smallint, workclass- Local-gov: smallint, workclass- Never-worked: smallint, workclass- Private: smallint, workclass- Self-emp-inc: smallint, workclass- Self-emp-not-inc: smallint, workclass- State-gov: smallint, workclass- Without-pay: smallint, fnlwgt: double, education- 10th: smallint, education- 11th: smallint, education- 12th: smallint, education- 1st-4th: smallint, education- 5th-6th: smallint, education- 7th-8th: smallint, education- 9th: smallint, education- Assoc-acdm: smallint, education- Assoc-voc: smallint, education- Bachelors: smallint, education- Doctorate: smallint, education- HS-grad: smallint, education- Masters: smallint, education- Preschool: smallint, education- Prof-school: smallint, education- Some-college: smallint, education-num: double, marital-status- Divorced: smallint, marital-status- Married-AF-spouse: smallint, marital-status- Married-civ-spouse: smallint, marital-status- Married-spouse-absent: smallint, marital-status- Never-married: smallint, marital-status- Separated: smallint, marital-status- Widowed: smallint, occupation- ?: smallint, occupation- Adm-clerical: smallint, occupation- Armed-Forces: smallint, occupation- Craft-repair: smallint, occupation- Exec-managerial: smallint, occupation- Farming-fishing: smallint, occupation- Handlers-cleaners: smallint, occupation- Machine-op-inspct: smallint, occupation- Other-service: smallint, occupation- Priv-house-serv: smallint, occupation- Prof-specialty: smallint, occupation- Protective-serv: smallint, occupation- Sales: smallint, occupation- Tech-support: smallint, occupation- Transport-moving: smallint, relationship- Husband: smallint, relationship- Not-in-family: smallint, relationship- Other-relative: smallint, relationship- Own-child: smallint, relationship- Unmarried: smallint, relationship- Wife: smallint, race- Amer-Indian-Eskimo: smallint, race- Asian-Pac-Islander: smallint, race- Black: smallint, race- Other: smallint, race- White: smallint, sex- Female: smallint, sex- Male: smallint, capital-gain: double, capital-loss: double, hours-per-week: double, country- ?: smallint, country- Cambodia: smallint, country- Canada: smallint, country- China: smallint, country- Columbia: smallint, country- Cuba: smallint, country- Dominican-Republic: smallint, country- Ecuador: smallint, country- El-Salvador: smallint, country- England: smallint, country- France: smallint, country- Germany: smallint, country- Greece: smallint, country- Guatemala: smallint, country- Haiti: smallint, country- Holand-Netherlands: smallint, country- Honduras: smallint, country- Hong: smallint, country- Hungary: smallint, country- India: smallint, country- Iran: smallint, country- Ireland: smallint, country- Italy: smallint, country- Jamaica: smallint, country- Japan: smallint, country- Laos: smallint, country- Mexico: smallint, country- Nicaragua: smallint, country- Outlying-US(Guam-USVI-etc): smallint, country- Peru: smallint, country- Philippines: smallint, country- Poland: smallint, country- Portugal: smallint, country- Puerto-Rico: smallint, country- Scotland: smallint, country- South: smallint, country- Taiwan: smallint, country- Thailand: smallint, country- Trinadad&Tobago: smallint, country- United-States: smallint, country- Vietnam: smallint, country- Yugoslavia: smallint, target: bigint, __index_level_0__: bigint]>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/06 04:28:40 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 3204856 ms exceeds timeout 120000 ms\n",
      "23/02/06 04:28:40 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "79c48e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-05 21:04:14.994412: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-05 21:04:19.564882: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 21:04:19 ERROR Executor: Exception in task 0.0 in stage 52.0 (TID 52)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1884, in __getitem__\n",
      "    idx = self.__fields__.index(item)\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n",
      "    return f(iterator)\n",
      "  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/3889536951.py\", line 17, in predict\n",
      "  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/3889536951.py\", line 17, in <listcomp>\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1889, in __getitem__\n",
      "    raise ValueError(item)\n",
      "ValueError: Index(['age', 'workclass- ?', 'workclass- Federal-gov', 'workclass- Local-gov',\n",
      "       'workclass- Never-worked', 'workclass- Private',\n",
      "       'workclass- Self-emp-inc', 'workclass- Self-emp-not-inc',\n",
      "       'workclass- State-gov', 'workclass- Without-pay',\n",
      "       ...\n",
      "       'country- Portugal', 'country- Puerto-Rico', 'country- Scotland',\n",
      "       'country- South', 'country- Taiwan', 'country- Thailand',\n",
      "       'country- Trinadad&Tobago', 'country- United-States',\n",
      "       'country- Vietnam', 'country- Yugoslavia'],\n",
      "      dtype='object', length=108)\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:831)\n",
      "23/02/05 21:04:19 WARN TaskSetManager: Lost task 0.0 in stage 52.0 (TID 52) (192.168.0.24 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1884, in __getitem__\n",
      "    idx = self.__fields__.index(item)\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n",
      "    return f(iterator)\n",
      "  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/3889536951.py\", line 17, in predict\n",
      "  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/3889536951.py\", line 17, in <listcomp>\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1889, in __getitem__\n",
      "    raise ValueError(item)\n",
      "ValueError: Index(['age', 'workclass- ?', 'workclass- Federal-gov', 'workclass- Local-gov',\n",
      "       'workclass- Never-worked', 'workclass- Private',\n",
      "       'workclass- Self-emp-inc', 'workclass- Self-emp-not-inc',\n",
      "       'workclass- State-gov', 'workclass- Without-pay',\n",
      "       ...\n",
      "       'country- Portugal', 'country- Puerto-Rico', 'country- Scotland',\n",
      "       'country- South', 'country- Taiwan', 'country- Thailand',\n",
      "       'country- Trinadad&Tobago', 'country- United-States',\n",
      "       'country- Vietnam', 'country- Yugoslavia'],\n",
      "      dtype='object', length=108)\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:831)\n",
      "\n",
      "23/02/05 21:04:19 ERROR TaskSetManager: Task 0 in stage 52.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 52.0 failed 1 times, most recent failure: Lost task 0.0 in stage 52.0 (TID 52) (192.168.0.24 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1884, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/3889536951.py\", line 17, in predict\n  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/3889536951.py\", line 17, in <listcomp>\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1889, in __getitem__\n    raise ValueError(item)\nValueError: Index(['age', 'workclass- ?', 'workclass- Federal-gov', 'workclass- Local-gov',\n       'workclass- Never-worked', 'workclass- Private',\n       'workclass- Self-emp-inc', 'workclass- Self-emp-not-inc',\n       'workclass- State-gov', 'workclass- Without-pay',\n       ...\n       'country- Portugal', 'country- Puerto-Rico', 'country- Scotland',\n       'country- South', 'country- Taiwan', 'country- Thailand',\n       'country- Trinadad&Tobago', 'country- United-States',\n       'country- Vietnam', 'country- Yugoslavia'],\n      dtype='object', length=108)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:831)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:831)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1884, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/3889536951.py\", line 17, in predict\n  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/3889536951.py\", line 17, in <listcomp>\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1889, in __getitem__\n    raise ValueError(item)\nValueError: Index(['age', 'workclass- ?', 'workclass- Federal-gov', 'workclass- Local-gov',\n       'workclass- Never-worked', 'workclass- Private',\n       'workclass- Self-emp-inc', 'workclass- Self-emp-not-inc',\n       'workclass- State-gov', 'workclass- Without-pay',\n       ...\n       'country- Portugal', 'country- Puerto-Rico', 'country- Scotland',\n       'country- South', 'country- Taiwan', 'country- Thailand',\n       'country- Trinadad&Tobago', 'country- United-States',\n       'country- Vietnam', 'country- Yugoslavia'],\n      dtype='object', length=108)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 23\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     return fm(iterator[FEATURES].values)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Use the mapPartitions function to make predictions on the data\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m predictions\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1197\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 52.0 failed 1 times, most recent failure: Lost task 0.0 in stage 52.0 (TID 52) (192.168.0.24 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1884, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/3889536951.py\", line 17, in predict\n  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/3889536951.py\", line 17, in <listcomp>\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1889, in __getitem__\n    raise ValueError(item)\nValueError: Index(['age', 'workclass- ?', 'workclass- Federal-gov', 'workclass- Local-gov',\n       'workclass- Never-worked', 'workclass- Private',\n       'workclass- Self-emp-inc', 'workclass- Self-emp-not-inc',\n       'workclass- State-gov', 'workclass- Without-pay',\n       ...\n       'country- Portugal', 'country- Puerto-Rico', 'country- Scotland',\n       'country- South', 'country- Taiwan', 'country- Thailand',\n       'country- Trinadad&Tobago', 'country- United-States',\n       'country- Vietnam', 'country- Yugoslavia'],\n      dtype='object', length=108)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:831)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:831)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1884, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/3889536951.py\", line 17, in predict\n  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/3889536951.py\", line 17, in <listcomp>\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1889, in __getitem__\n    raise ValueError(item)\nValueError: Index(['age', 'workclass- ?', 'workclass- Federal-gov', 'workclass- Local-gov',\n       'workclass- Never-worked', 'workclass- Private',\n       'workclass- Self-emp-inc', 'workclass- Self-emp-not-inc',\n       'workclass- State-gov', 'workclass- Without-pay',\n       ...\n       'country- Portugal', 'country- Puerto-Rico', 'country- Scotland',\n       'country- South', 'country- Taiwan', 'country- Thailand',\n       'country- Trinadad&Tobago', 'country- United-States',\n       'country- Vietnam', 'country- Yugoslavia'],\n      dtype='object', length=108)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "FEATURES = x_test.columns\n",
    "path = './weights/weights-epoch(10)-batch(256)-embedding(5).h5' \n",
    "\n",
    "# Load the data into a PySpark DataFrame\n",
    "df = spark.read.parquet('data/test.parquet', header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# Define the function that will make predictions with the model\n",
    "def predict(iterator):\n",
    "    # model load\n",
    "    fm = DeepFM(embedding_size=config.EMBEDDING_SIZE, num_feature=len(field_index),\n",
    "               num_field=len(field_dict), field_index=field_index)\n",
    "    fm.build(input_shape = (1,len(field_index)))\n",
    "    fm.load_weights(path)\n",
    "\n",
    "    # Make predictions for each row in the iterator\n",
    "    results = [float(fm(row[FEATURES].values)) for row in iterator]\n",
    "    return results\n",
    "\n",
    "#     return fm(iterator[FEATURES].values)\n",
    "\n",
    "# Use the mapPartitions function to make predictions on the data\n",
    "predictions = df.rdd.mapPartitions(predict).collect()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a77672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa509a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fd3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca46e368",
   "metadata": {},
   "source": [
    "### MapPartitions example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d55ae20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n",
      "|firstname|lastname|gender|salary|\n",
      "+---------+--------+------+------+\n",
      "|    James|   Smith|     M|  3000|\n",
      "|     Anna|    Rose|     F|  4100|\n",
      "|   Robert|Williams|     M|  6200|\n",
      "+---------+--------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data = [('James','Smith','M',3000),\n",
    "  ('Anna','Rose','F',4100),\n",
    "  ('Robert','Williams','M',6200), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fea18315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           name|bonus|\n",
      "+---------------+-----+\n",
      "|    James,Smith|300.0|\n",
      "|      Anna,Rose|410.0|\n",
      "|Robert,Williams|620.0|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This function calls for each partition\n",
    "def reformat(partitionData):\n",
    "    for row in partitionData:\n",
    "        yield [row['firstname']+\",\"+row.lastname,row.salary*10/100]\n",
    "\n",
    "df2=df.rdd.mapPartitions(reformat).toDF([\"name\",\"bonus\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded9951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
