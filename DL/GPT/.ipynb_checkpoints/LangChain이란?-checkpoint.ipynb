{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d607596e",
   "metadata": {},
   "source": [
    "# LangChain \n",
    "* 공부를 하고 있는 지금도 정말 많은 LLM모델과 그것을 이용&활용한 파생된 모델들이 나오고 있다. 그런점에서 공부 할 것들도 점점 많아지는 기분 좋은? 상황이다.\n",
    "* 이런 상황에서 먼저 LangChain을 공부하려고 한다\n",
    "\n",
    "## LangChain 모듈 \n",
    "> ##### 0. Document Loaders : 파일 형태의 데이터를 load해주는 역할\n",
    "> ##### 1. LLMs: LLM에 input을 넣어 prediction\n",
    "> ##### 2. Prompt Template: prompt 매니징\n",
    "> ##### 3. Chains: LLM과 prompt의 결합체들로 multi-step workflow를 구성\n",
    "> ##### 4. Agents: 유저 인풋에 따라 Chain을 ‘동적’으로 call. 여기서 ‘동적’이라는 의미는 순서가 정해져있지 않다는 것\n",
    "> ##### 5. Memory: Chain, Agent에 State를 추가\n",
    "\n",
    "여기서 LLM, Prompt보다는 Chains, Agents, Memory가 LangChain의 핵심 block이라고 할 수 있다.  \n",
    "\n",
    " \n",
    "\n",
    "그 중에서도 Chain이 LangChain 이름 자체가 암시하는 것처럼 LLM기반 Application의 큰 구조물(?)이라고 할 수 있다. Chain을 여러 개를 엮어 Application을 구성하는 방식이라 생각하면 된다.  \n",
    "\n",
    " \n",
    "\n",
    "우선 LangChain이 제공하고 있는 모듈에 대해 한 눈으로 크게 살펴본 뒤, 세세하게 하나씩 설명을 읽으면 이 라이브러리가 전반적으로 어떤 영역을 커버해줄 수 있는지에 대해 쉽게 파악할 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96f9ba6",
   "metadata": {},
   "source": [
    "## 프로세스\n",
    "1. 데이터 로딩(loader & indexes)\n",
    "2. 프롬프트 만들기 (prompt)\n",
    "3. LLM 모델 생성 (llms)\n",
    "4. 이들을 연결 (chains)\n",
    "5. 이전 대화를 기억하여 답변을 줄 수 있는 메모리를 생성 (memory)\n",
    "6. 그 외 기타 기능(검색/연산 등)을 붙여 Agent 생성 (agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ac815",
   "metadata": {},
   "source": [
    "## Tutorial Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95ade8",
   "metadata": {},
   "source": [
    "##### - GPT4ALL을 이용한 답변 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8b732c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:08:47.502614Z",
     "start_time": "2023-05-29T08:07:35.888824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file.\n",
      "gptj_model_load: loading model from '/Users/youngyong/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\n",
      "gptj_model_load: n_vocab = 50400\n",
      "gptj_model_load: n_ctx   = 2048\n",
      "gptj_model_load: n_embd  = 4096\n",
      "gptj_model_load: n_head  = 16\n",
      "gptj_model_load: n_layer = 28\n",
      "gptj_model_load: n_rot   = 64\n",
      "gptj_model_load: f16     = 2\n",
      "gptj_model_load: ggml ctx size = 5401.45 MB\n",
      "gptj_model_load: kv self size  =  896.00 MB\n",
      "gptj_model_load: ................................... done\n",
      "gptj_model_load: model size =  3609.38 MB / num tensors = 285\n",
      "### Instruction: \n",
      "            The prompt below is a question to answer, a task to complete, or a conversation \n",
      "            to respond to; decide which and write an appropriate response.\n",
      "            \n",
      "### Prompt: \n",
      "What is a good name for a company that makes colorful socks?\n",
      "### Response:\n",
      " How about \"Colorful Socks Inc.\"?\n",
      "{'model': 'ggml-gpt4all-j-v1.3-groovy', 'usage': {'prompt_tokens': 286, 'completion_tokens': 33, 'total_tokens': 319}, 'choices': [{'message': {'role': 'assistant', 'content': ' How about \"Colorful Socks Inc.\"?'}}]}\n"
     ]
    }
   ],
   "source": [
    "import gpt4all\n",
    "\n",
    "gptj = gpt4all.GPT4All(\"ggml-gpt4all-j-v1.3-groovy\")\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is a good name for a company that makes colorful socks?\"}]\n",
    "ret = gptj.chat_completion(messages)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee0350",
   "metadata": {},
   "source": [
    "##### LangChain을 통한 답변 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3692d8e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:10:07.198638Z",
     "start_time": "2023-05-29T08:09:36.921733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file.\n",
      "gptj_model_load: loading model from '/Users/youngyong/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\n",
      "gptj_model_load: n_vocab = 50400\n",
      "gptj_model_load: n_ctx   = 2048\n",
      "gptj_model_load: n_embd  = 4096\n",
      "gptj_model_load: n_head  = 16\n",
      "gptj_model_load: n_layer = 28\n",
      "gptj_model_load: n_rot   = 64\n",
      "gptj_model_load: f16     = 2\n",
      "gptj_model_load: ggml ctx size = 5401.45 MB\n",
      "gptj_model_load: kv self size  =  896.00 MB\n",
      "gptj_model_load: ................................... done\n",
      "gptj_model_load: model size =  3609.38 MB / num tensors = 285\n",
      "How about 'ColorFunk' or something similar to incorporate the word 'funky'?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"How about 'ColorFunk' or something similar to incorporate the word 'funky'?\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "\n",
    "local_path = '/Users/youngyong/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin' \n",
    "llm = GPT4All(model=local_path, verbose=True)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt) # Prompt + LLM을 결합하는 LLMChain\n",
    "chain.run(\"colorful socks\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b67f76",
   "metadata": {},
   "source": [
    "##### callback 함수 추가후 출력값 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "554bd1fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:52:02.444755Z",
     "start_time": "2023-05-29T08:51:38.924924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file.\n",
      "gptj_model_load: loading model from '/Users/youngyong/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\n",
      "gptj_model_load: n_vocab = 50400\n",
      "gptj_model_load: n_ctx   = 2048\n",
      "gptj_model_load: n_embd  = 4096\n",
      "gptj_model_load: n_head  = 16\n",
      "gptj_model_load: n_layer = 28\n",
      "gptj_model_load: n_rot   = 64\n",
      "gptj_model_load: f16     = 2\n",
      "gptj_model_load: ggml ctx size = 5401.45 MB\n",
      "gptj_model_load: kv self size  =  896.00 MB\n",
      "gptj_model_load: ................................... done\n",
      "gptj_model_load: model size =  3609.38 MB / num tensors = 285\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is a good name for a company that makes colorful socks?\u001b[0m\n",
      "How about \"Colorful Socks?\"\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How about \"Colorful Socks?\"'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "local_path = '/Users/youngyong/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin' \n",
    "llm = GPT4All(model=local_path, verbose=True)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "handler = StdOutCallbackHandler()\n",
    "chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])\n",
    "chain.run(\"colorful socks\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3249d",
   "metadata": {},
   "source": [
    "##### template을 통해 Answer까지 통제가 가능하다 !! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a163cb80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T09:18:04.816054Z",
     "start_time": "2023-05-29T09:14:52.740503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file.\n",
      "gptj_model_load: loading model from '/Users/youngyong/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\n",
      "gptj_model_load: n_vocab = 50400\n",
      "gptj_model_load: n_ctx   = 2048\n",
      "gptj_model_load: n_embd  = 4096\n",
      "gptj_model_load: n_head  = 16\n",
      "gptj_model_load: n_layer = 28\n",
      "gptj_model_load: n_rot   = 64\n",
      "gptj_model_load: f16     = 2\n",
      "gptj_model_load: ggml ctx size = 5401.45 MB\n",
      "gptj_model_load: kv self size  =  896.00 MB\n",
      "gptj_model_load: ................................... done\n",
      "gptj_model_load: model size =  3609.38 MB / num tensors = 285\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What is a good name for a company that makes colarful Socks?\n",
      "\n",
      "Answer: Let's think step by step on it.\n",
      "\n",
      "\u001b[0m\n",
      " 1) The first thing we need to do after thinking about the brand essence and value proposition, in order of priority would be :  - Researching competitors names within your industry or similar businesses already established (to find inspiration). And also exploring other options that you may have come across while brainstorming such as domain name ideas.2) After researching competitor's brands we can start to think about what differentiates our brand from others in the market, and this could include things like a unique taste profile or packaging design, use of natural ingredients (if any), etc.. Also by doing some research on consumer preferences within your target audience you may be able identify which aspects they are most interested in when it comes to beverages.3) We can also think about branding guidelines such as logo and color schemes based off the brand essence we identified earlier, along with a clear description of our product (in this case cola). \n",
      " 4) The next step is coming up with ideas for names that would be appealing on different levels - easy to remember/memorize or unique sounding.5) Once you have finalized and choose your name it's time then start developing an interesting story behind the brand, how we came about this idea...and most importantly why YOU as a company\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" 1) The first thing we need to do after thinking about the brand essence and value proposition, in order of priority would be :  - Researching competitors names within your industry or similar businesses already established (to find inspiration). And also exploring other options that you may have come across while brainstorming such as domain name ideas.2) After researching competitor's brands we can start to think about what differentiates our brand from others in the market, and this could include things like a unique taste profile or packaging design, use of natural ingredients (if any), etc.. Also by doing some research on consumer preferences within your target audience you may be able identify which aspects they are most interested in when it comes to beverages.3) We can also think about branding guidelines such as logo and color schemes based off the brand essence we identified earlier, along with a clear description of our product (in this case cola). \\n 4) The next step is coming up with ideas for names that would be appealing on different levels - easy to remember/memorize or unique sounding.5) Once you have finalized and choose your name it's time then start developing an interesting story behind the brand, how we came about this idea...and most importantly why YOU as a company\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "local_path = '/Users/youngyong/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin' \n",
    "llm = GPT4All(model=local_path, verbose=True)\n",
    "\n",
    "handler = StdOutCallbackHandler()\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step on it.\n",
    "\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm,callbacks=[handler])\n",
    "\n",
    "\n",
    "question = \"What is a good name for a company that makes colarful Socks?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c21bb7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T09:06:18.008670Z",
     "start_time": "2023-05-29T09:03:05.266286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file.\n",
      "gptj_model_load: loading model from '/Users/youngyong/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\n",
      "gptj_model_load: n_vocab = 50400\n",
      "gptj_model_load: n_ctx   = 2048\n",
      "gptj_model_load: n_embd  = 4096\n",
      "gptj_model_load: n_head  = 16\n",
      "gptj_model_load: n_layer = 28\n",
      "gptj_model_load: n_rot   = 64\n",
      "gptj_model_load: f16     = 2\n",
      "gptj_model_load: ggml ctx size = 5401.45 MB\n",
      "gptj_model_load: kv self size  =  896.00 MB\n",
      "gptj_model_load: ................................... done\n",
      "gptj_model_load: model size =  3609.38 MB / num tensors = 285\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What Formula 1 pilot won the championship in the year Leonardo di Caprio was born?\n",
      "\n",
      "Answer: Let's think step by step on it.\n",
      "\n",
      "\u001b[0m\n",
      " Step I : We need to know which driver is referred as \"Leonardo DiCaprio\"  and his birthdate, so we can go look for any records related with F1 championships he might have won or been a part of winning titles in the year mentioned aboveStep II: In order to find out who was racing at that time Leonardo Dicaprio's birth date could be found. Once I got this information from there ,we need to compare it by his name and see if any other records were available related with F1 championships he might have won or been a part of winning titles in the year mentioned aboveStep III : After finding out who was racing at that time, we should find which driver has same birthdate as Leonardo DiCaprio's. This will give us information about when they came into picture and start looking for any records related with F1 championships he might have won or been a part of winning titles in the year mentioned aboveStep IV: After finding out who was racing at that time, we should find which driver has same birthdate as Leonardo DiCaprio's. This will give us information about when they came into picture and start looking for any records related with F1 championships he might have won or been a part of winning\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Step I : We need to know which driver is referred as \"Leonardo DiCaprio\"  and his birthdate, so we can go look for any records related with F1 championships he might have won or been a part of winning titles in the year mentioned aboveStep II: In order to find out who was racing at that time Leonardo Dicaprio\\'s birth date could be found. Once I got this information from there ,we need to compare it by his name and see if any other records were available related with F1 championships he might have won or been a part of winning titles in the year mentioned aboveStep III : After finding out who was racing at that time, we should find which driver has same birthdate as Leonardo DiCaprio\\'s. This will give us information about when they came into picture and start looking for any records related with F1 championships he might have won or been a part of winning titles in the year mentioned aboveStep IV: After finding out who was racing at that time, we should find which driver has same birthdate as Leonardo DiCaprio\\'s. This will give us information about when they came into picture and start looking for any records related with F1 championships he might have won or been a part of winning'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "local_path = '/Users/youngyong/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin' \n",
    "llm = GPT4All(model=local_path, verbose=True)\n",
    "\n",
    "handler = StdOutCallbackHandler()\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step on it.\n",
    "\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm,callbacks=[handler])\n",
    "\n",
    "# Hardcoded question\n",
    "question = \"What Formula 1 pilot won the championship in the year Leonardo di Caprio was born?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f357063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22869463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3884c417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfe180e3",
   "metadata": {},
   "source": [
    "##### memory - 이전의 대화내용을 기억할 수 있게하는 기능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76cc72dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:24:44.458501Z",
     "start_time": "2023-05-29T08:23:24.200575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file.\n",
      "gptj_model_load: loading model from '/Users/youngyong/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\n",
      "gptj_model_load: n_vocab = 50400\n",
      "gptj_model_load: n_ctx   = 2048\n",
      "gptj_model_load: n_embd  = 4096\n",
      "gptj_model_load: n_head  = 16\n",
      "gptj_model_load: n_layer = 28\n",
      "gptj_model_load: n_rot   = 64\n",
      "gptj_model_load: f16     = 2\n",
      "gptj_model_load: ggml ctx size = 5401.45 MB\n",
      "gptj_model_load: kv self size  =  896.00 MB\n",
      "gptj_model_load: ................................... done\n",
      "gptj_model_load: model size =  3609.38 MB / num tensors = 285\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: What is a good name for a company that makes colorful socks?\n",
      "AI:\u001b[0m\n",
      " I am sorry but without more information about your business or industry there isn't any context based on which color of sock you want me to suggest as the best one for my customers.\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I am sorry but without more information about your business or industry there isn't any context based on which color of sock you want me to suggest as the best one for my customers.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "local_path = '/Users/youngyong/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin' \n",
    "llm = GPT4All(model=local_path, verbose=True)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"What is a good name for a company that makes colorful socks?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7b14807",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:28:54.192758Z",
     "start_time": "2023-05-29T08:26:45.593836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What is a good name for a company that makes colorful socks?\n",
      "AI:  I am sorry but without more information about your business or industry there isn't any context based on which color of sock you want me to suggest as the best one for my customers.\n",
      "Human: No, it's not about sock colors, it's about naming a sock company. Send me an example of a colorful socks name company\n",
      "AI:\u001b[0m\n",
      " I apologize but without more information and specifics such like what type or style of shoes does your target customer wear ,what is their color preferences as well the current fashion trends which could influence on choosing this kind of products .I think there's no one-size fits all answer for naming companies.\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I apologize but without more information and specifics such like what type or style of shoes does your target customer wear ,what is their color preferences as well the current fashion trends which could influence on choosing this kind of products .I think there's no one-size fits all answer for naming companies.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"No, it's not about sock colors, it's about naming a sock company. Send me an example of a colorful socks name company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b313a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e4e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc96f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6040b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df71410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8141c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23721d35",
   "metadata": {},
   "source": [
    "참고 영역 \n",
    "- Langchain 공홈 / https://docs.langchain.com/docs/\n",
    "- Langchain이란? / https://julie-tech.tistory.com/138\n",
    "- Langchain 설명 / https://revf.tistory.com/280\n",
    "- Faiss 유사도 검색 / https://lsjsj92.tistory.com/605\n",
    "- LangChain & GPT4All 활용 예시 / https://artificialcorner.com/gpt4all-is-the-local-chatgpt-for-your-documents-and-it-is-free-df1016bc335\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4f4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
