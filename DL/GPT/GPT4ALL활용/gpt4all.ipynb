{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e03a391f",
   "metadata": {},
   "source": [
    "# Env Setting\n",
    "아나콘다 준비 -> 터미널 열기  \n",
    "conda create -n gpt python=3.10 anaconda  \n",
    "conda activate gpt  \n",
    "pip install -r requirements.txt  \n",
    "jupyter notebook\n",
    "  \n",
    "Model download  \n",
    "GPT4ALLModel_down_load url :   아래 코드 참고  \n",
    "EmbeddingModel_down_load url :   https://huggingface.co/Pi3141/alpaca-native-7B-ggml/tree/397e872bf4c83f4c642317a5bf65ce84a105786e\n",
    "\n",
    "*.bin으로 끝나는 파일 다운 받으면됨  \n",
    "  \n",
    "그리고 pdf file과 model path 자신의 path에 맞게 수정해주기 ~  \n",
    "mac 최신 버전으로 업데이트도 해주기 ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d71d7ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T06:24:07.805303Z",
     "start_time": "2023-06-04T06:23:59.171053Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "# function for loading only TXT files\n",
    "from langchain.document_loaders import TextLoader\n",
    "# text splitter for create chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# to be able to load the pdf files\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "# Vector Store Index to create our database about our knowledge\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "# LLamaCpp embeddings from the Alpaca model\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "# FAISS  library for similaarity search\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "import os  #for interaaction with the files\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e01867d",
   "metadata": {},
   "source": [
    "# Model Load\n",
    "- GPT4ALL model\n",
    "- Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e39243e",
   "metadata": {},
   "source": [
    "Down load GPT4ALL model \n",
    "- 아래 코드가 모델 다운하는 코드임 !\n",
    "- 다운 다되면 모델 path가 보임!, 그것을 path에 추가해서 활용해 주자\n",
    "- 다운중에 에러나면 모델이 제대로 작동하지 않음. 에러가 난다면 해당 파일을 삭제후에 다시 다운하기!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c67642e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T06:24:15.655076Z",
     "start_time": "2023-06-04T06:24:07.810600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file.\n",
      "gptj_model_load: loading model from '/Users/youngyong/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\n",
      "gptj_model_load: n_vocab = 50400\n",
      "gptj_model_load: n_ctx   = 2048\n",
      "gptj_model_load: n_embd  = 4096\n",
      "gptj_model_load: n_head  = 16\n",
      "gptj_model_load: n_layer = 28\n",
      "gptj_model_load: n_rot   = 64\n",
      "gptj_model_load: f16     = 2\n",
      "gptj_model_load: ggml ctx size = 5401.45 MB\n",
      "gptj_model_load: kv self size  =  896.00 MB\n",
      "gptj_model_load: ................................... done\n",
      "gptj_model_load: model size =  3609.38 MB / num tensors = 285\n"
     ]
    }
   ],
   "source": [
    "import gpt4all\n",
    "\n",
    "gptj = gpt4all.GPT4All(\"ggml-gpt4all-j-v1.3-groovy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d068929a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T06:25:32.469968Z",
     "start_time": "2023-06-04T06:25:25.332889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file.\n",
      "gptj_model_load: loading model from '/Users/youngyong/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\n",
      "gptj_model_load: n_vocab = 50400\n",
      "gptj_model_load: n_ctx   = 2048\n",
      "gptj_model_load: n_embd  = 4096\n",
      "gptj_model_load: n_head  = 16\n",
      "gptj_model_load: n_layer = 28\n",
      "gptj_model_load: n_rot   = 64\n",
      "gptj_model_load: f16     = 2\n",
      "gptj_model_load: ggml ctx size = 5401.45 MB\n",
      "gptj_model_load: kv self size  =  896.00 MB\n",
      "gptj_model_load: ................................... done\n",
      "gptj_model_load: model size =  3609.38 MB / num tensors = 285\n"
     ]
    }
   ],
   "source": [
    "# 모델 다운한 path넣기 ~\n",
    "gpt4all_path = '/Users/youngyong/.cache/gpt4all/ggml-gpt4all-j-v1.3-groovy.bin' \n",
    "llama_path = './models/ggml-model-q4_0.bin' \n",
    "# Calback manager for handling the calls with  the model\n",
    "handler = StdOutCallbackHandler()\n",
    "# create the GPT4All llm object\n",
    "llm = GPT4All(model=gpt4all_path, callbacks=[handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a93152c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T06:25:41.227540Z",
     "start_time": "2023-06-04T06:25:34.690805Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./models/ggml-model-q4_0.bin\n",
      "llama.cpp: can't use mmap because tensors are not aligned; convert to new format to avoid this\n",
      "llama_model_load_internal: format     = 'ggml' (old version with low tokenizer quality and no mmap support)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size = 4113748.20 KB\n",
      "llama_model_load_internal: mem required  = 5809.33 MB (+ 2052.00 MB per state)\n",
      "...................................................................................................\n",
      ".\n",
      "llama_init_from_file: kv self size  =  512.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# create the embedding object\n",
    "embeddings = LlamaCppEmbeddings(model_path=llama_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc8a0f",
   "metadata": {},
   "source": [
    "# Data Load & Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b049da18",
   "metadata": {},
   "source": [
    "- 함수 선언\n",
    "> split_chunks : 사이즈별로 파일 나누기  \n",
    "> create_index : text의 chunk를 embedding화  \n",
    "> similarity_search : 질문과 가장 유사한 word 뽑아내기  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6f49463",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T01:02:08.781183Z",
     "start_time": "2023-05-31T01:02:08.771942Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split text \n",
    "def split_chunks(sources):\n",
    "    chunks = []\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)\n",
    "    for chunk in splitter.split_documents(sources):\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def create_index(chunks):\n",
    "    texts = [doc.page_content for doc in chunks]\n",
    "    metadatas = [doc.metadata for doc in chunks]\n",
    "\n",
    "    search_index = FAISS.from_texts(texts, embeddings, metadatas=metadatas)\n",
    "\n",
    "    return search_index\n",
    "\n",
    "\n",
    "def similarity_search(query, index):\n",
    "    # k is the number of similarity searched that matches the query\n",
    "    # default is 4\n",
    "    matched_docs = index.similarity_search(query, k=3) \n",
    "    sources = []\n",
    "    for doc in matched_docs:\n",
    "        sources.append(\n",
    "            {\n",
    "                \"page_content\": doc.page_content,\n",
    "                \"metadata\": doc.metadata,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return matched_docs, sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4095a2fd",
   "metadata": {},
   "source": [
    "- PDF load -> Text 변환 -> Chunk 사이즈로 나누기 -> 임베딩 벡터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cada41c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T02:36:41.031728Z",
     "start_time": "2023-05-31T01:02:08.819411Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the loop...\n",
      "generating fist vector database and then iterate with .merge_from\n",
      "Main Vector database created. Start iteration and merging...\n",
      "AirPodsPro.pdf\n",
      "embedding file nm :  AirPodsPro\n",
      "loop position 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 78401.35 ms /   221 tokens (  354.76 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 78528.02 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 66403.24 ms /   194 tokens (  342.28 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 66512.55 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 43129.09 ms /   125 tokens (  345.03 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 43200.43 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 43186.76 ms /   126 tokens (  342.75 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 43258.45 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 47595.43 ms /   136 tokens (  349.97 ms per token)\n",
      "llama_print_timings:        eval time =   455.37 ms /     1 runs   (  455.37 ms per run)\n",
      "llama_print_timings:       total time = 48135.85 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 43509.21 ms /   125 tokens (  348.07 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 43582.97 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 49527.89 ms /   143 tokens (  346.35 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 49611.27 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 29954.52 ms /    86 tokens (  348.31 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 30005.59 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 105591.00 ms /   301 tokens (  350.80 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 105765.99 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 56309.42 ms /   164 tokens (  343.35 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 56406.77 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 58323.71 ms /   170 tokens (  343.08 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 58425.21 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 54649.86 ms /   160 tokens (  341.56 ms per token)\n",
      "llama_print_timings:        eval time =   426.64 ms /     1 runs   (  426.64 ms per run)\n",
      "llama_print_timings:       total time = 55172.69 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 43414.59 ms /   128 tokens (  339.18 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 43488.87 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 56702.87 ms /   165 tokens (  343.65 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 56800.04 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 15960.81 ms /    47 tokens (  339.59 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 15988.93 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 28897.13 ms /    85 tokens (  339.97 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 28947.70 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 98043.88 ms /   276 tokens (  355.23 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 98210.79 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 107754.84 ms /   306 tokens (  352.14 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 107936.23 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 103702.13 ms /   298 tokens (  347.99 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 103880.39 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 82976.60 ms /   237 tokens (  350.11 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 83117.35 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 87804.98 ms /   251 tokens (  349.82 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 87953.81 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 92059.33 ms /   261 tokens (  352.72 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 92214.71 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start merging with db0...\n",
      "completed in 0:23:17.574004\n",
      "-----------------------------------\n",
      "AirPods.pdf\n",
      "embedding file nm :  AirPods\n",
      "loop position 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 94901.77 ms /   224 tokens (  423.67 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 95137.95 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 81623.55 ms /   211 tokens (  386.84 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 81797.32 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 53851.40 ms /   142 tokens (  379.24 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 53971.85 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 51060.76 ms /   139 tokens (  367.34 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 51168.80 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 53301.06 ms /   144 tokens (  370.15 ms per token)\n",
      "llama_print_timings:        eval time =   450.60 ms /     1 runs   (  450.60 ms per run)\n",
      "llama_print_timings:       total time = 53850.57 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 55330.70 ms /   146 tokens (  378.98 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 55437.08 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 62117.62 ms /   166 tokens (  374.20 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 62236.28 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 24072.20 ms /    68 tokens (  354.00 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 24130.73 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 95266.53 ms /   268 tokens (  355.47 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 95431.37 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 53599.78 ms /   154 tokens (  348.05 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 53692.45 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 47394.51 ms /   135 tokens (  351.07 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 47494.40 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 39166.03 ms /   111 tokens (  352.85 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 39251.46 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 49663.40 ms /   143 tokens (  347.30 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 49753.64 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 57858.47 ms /   166 tokens (  348.54 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 57954.82 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 41514.97 ms /   118 tokens (  351.82 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 41589.81 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 51386.61 ms /   146 tokens (  351.96 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 51481.07 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 60594.56 ms /   176 tokens (  344.29 ms per token)\n",
      "llama_print_timings:        eval time =   450.51 ms /     1 runs   (  450.51 ms per run)\n",
      "llama_print_timings:       total time = 61159.05 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 43289.01 ms /   120 tokens (  360.74 ms per token)\n",
      "llama_print_timings:        eval time =   449.40 ms /     1 runs   (  449.40 ms per run)\n",
      "llama_print_timings:       total time = 43821.85 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 86128.47 ms /   230 tokens (  374.47 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 86277.34 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 29503.91 ms /    83 tokens (  355.47 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 29562.05 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 105873.16 ms /   283 tokens (  374.11 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 106056.65 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 116330.87 ms /   306 tokens (  380.17 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 116522.10 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 115111.71 ms /   298 tokens (  386.28 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 115300.15 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 82779.93 ms /   237 tokens (  349.28 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 82918.09 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 88237.68 ms /   251 tokens (  351.54 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 88390.45 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 99502.86 ms /   261 tokens (  381.24 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 99662.77 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 87699.04 ms /   226 tokens (  388.05 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 87856.29 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 57550.00 ms /   164 tokens (  350.91 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 57648.16 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 50638.25 ms /   144 tokens (  351.65 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 50723.88 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 57213.51 ms /   154 tokens (  371.52 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 57315.45 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 52099.81 ms /   142 tokens (  366.90 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 52185.22 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 40416.54 ms /   115 tokens (  351.45 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 40493.81 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 42606.10 ms /   119 tokens (  358.03 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 42678.79 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 63249.31 ms /   146 tokens (  433.21 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 63350.76 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  5434.30 ms /    16 tokens (  339.64 ms per token)\n",
      "llama_print_timings:        eval time =   469.29 ms /     1 runs   (  469.29 ms per run)\n",
      "llama_print_timings:       total time =  5918.10 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 50432.47 ms /   146 tokens (  345.43 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 50522.87 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 62534.39 ms /   183 tokens (  341.72 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 62642.16 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 62320.17 ms /   181 tokens (  344.31 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 62428.10 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 71055.08 ms /   204 tokens (  348.31 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 71183.60 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 75170.69 ms /   213 tokens (  352.91 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 75305.28 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 71161.78 ms /   204 tokens (  348.83 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 71285.23 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 66627.36 ms /   189 tokens (  352.53 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 66750.94 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 69494.20 ms /   202 tokens (  344.03 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 69616.50 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 51280.73 ms /   148 tokens (  346.49 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 51376.40 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start merging with db0...\n",
      "completed in 0:46:23.977213\n",
      "-----------------------------------\n",
      "AirPodsMax.pdf\n",
      "embedding file nm :  AirPodsMax\n",
      "loop position 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 67794.80 ms /   200 tokens (  338.97 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 67906.52 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 71081.89 ms /   208 tokens (  341.74 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 71198.69 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 65205.08 ms /   152 tokens (  428.98 ms per token)\n",
      "llama_print_timings:        eval time =   450.33 ms /     1 runs   (  450.33 ms per run)\n",
      "llama_print_timings:       total time = 65751.59 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 54389.52 ms /   130 tokens (  418.38 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 54477.17 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 62295.62 ms /   139 tokens (  448.17 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 62392.44 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 51431.75 ms /   150 tokens (  342.88 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 51519.32 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 35309.84 ms /   104 tokens (  339.52 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 35369.61 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 43600.80 ms /   125 tokens (  348.81 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 43676.25 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 29001.53 ms /    84 tokens (  345.26 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 29053.39 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 97170.85 ms /   276 tokens (  352.07 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 97334.17 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 106400.69 ms /   306 tokens (  347.71 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 106582.60 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 102376.55 ms /   298 tokens (  343.55 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 102552.27 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 81745.93 ms /   237 tokens (  344.92 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 81886.54 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 87102.76 ms /   251 tokens (  347.02 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 87252.80 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 91749.36 ms /   261 tokens (  351.53 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 91902.72 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 82197.96 ms /   237 tokens (  346.83 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 82334.23 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 55732.74 ms /   156 tokens (  357.26 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 55826.47 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 58233.66 ms /   168 tokens (  346.63 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 58330.85 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 80301.83 ms /   228 tokens (  352.20 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 80437.66 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 69649.15 ms /   203 tokens (  343.10 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 69770.69 ms\n",
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 70301.54 ms /   204 tokens (  344.62 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 70423.28 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start merging with db0...\n",
      "completed in 0:24:50.582939\n",
      "-----------------------------------\n",
      "All documents processed in 1:34:32.203047\n",
      "the daatabase is done with 3 subset of db index\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3899.34 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 23934.29 ms /    70 tokens (  341.92 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 23976.22 ms\n"
     ]
    }
   ],
   "source": [
    "# get the list of pdf files from the docs directory into a list  format\n",
    "pdf_folder_path = './docs'\n",
    "doc_list = [s for s in os.listdir(pdf_folder_path) if s.endswith('.pdf')]\n",
    "num_of_docs = len(doc_list)\n",
    "\n",
    "# create a loader for the PDFs from the path\n",
    "general_start = datetime.datetime.now() #not used now but useful\n",
    "print(\"starting the loop...\")\n",
    "loop_start = datetime.datetime.now() #not used now but useful\n",
    "print(\"generating fist vector database and then iterate with .merge_from\")\n",
    "# loader = PyPDFLoader(os.path.join(pdf_folder_path, doc_list[0]))\n",
    "# docs = loader.load()\n",
    "# chunks = split_chunks(docs)\n",
    "# db0 = create_index(chunks)\n",
    "print(\"Main Vector database created. Start iteration and merging...\")\n",
    "for i in range(num_of_docs):\n",
    "    print(doc_list[i])\n",
    "    print('embedding file nm : ',doc_list[i].split('.')[0])\n",
    "    print(f\"loop position {i}\")\n",
    "    loader = PyPDFLoader(os.path.join(pdf_folder_path, doc_list[i]))\n",
    "    start = datetime.datetime.now() #not used now but useful\n",
    "    docs = loader.load()\n",
    "    chunks = split_chunks(docs)\n",
    "    dbi = create_index(chunks)\n",
    "    print(\"start merging with db0...\")\n",
    "    dbi.save_local(doc_list[i].split('.')[0])\n",
    "    end = datetime.datetime.now() #not used now but useful\n",
    "    elapsed = end - start #not used now but useful\n",
    "    #total time\n",
    "    print(f\"completed in {elapsed}\")\n",
    "    print(\"-----------------------------------\")\n",
    "loop_end = datetime.datetime.now() #not used now but useful\n",
    "loop_elapsed = loop_end - loop_start #not used now but useful\n",
    "print(f\"All documents processed in {loop_elapsed}\")\n",
    "print(f\"the daatabase is done with {num_of_docs} subset of db index\")\n",
    "print(\"-----------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baaae1f",
   "metadata": {},
   "source": [
    "### Faiss를 통한 query와 파일간의 유사도가 높은 문장 뽑아오기\n",
    "- 에어팟, 에어팟프로, 에어팟맥스 pdf파일의 text와 유사도 검사 진행  \n",
    "> 1. query = \"product's brief explanation, does not contain the word wikipedia\"  \n",
    "> 2. 각 파일의 유사도가 높은 문장을 가져옴 (아래에서 확인)\n",
    "\n",
    "문제? : 정의한 query의 문제인지 가져오는 문장이 품질이 좋지 않은 이슈 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f74766b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T03:49:27.585493Z",
     "start_time": "2023-05-31T03:49:10.335865Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3827.46 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  6442.64 ms /    16 tokens (  402.67 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  6453.17 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question is: product's brief explanation, does not contain the word wikipedia\n",
      "Here the result of the semantic search on the index, without GPT4All..\n",
      "accelerometers  and optical senso rs that can detec t presses on the stemand in-ear placement, and automati c pausing when they are taken out ofthe ears. Control by tapping is repla ced by pressing a force sensor on thestems.[ 7 ] They are rated IPX4  for water resistance.[ 3]The AirPods Pro use the H1 chip  also found in the second and third-generation AirPods, that supports hands-free \"Hey Siri\". They have activenoise cancellation , accomplished by microphones detecting outside sound\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3827.46 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  5348.84 ms /    16 tokens (  334.30 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  5357.45 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question is: product's brief explanation, does not contain the word wikipedia\n",
      "Here the result of the semantic search on the index, without GPT4All..\n",
      "with each other. Having earphones in is generally perceived as a \"do not disturb\" signal, and some AirPods usersuse this fact to strategically avoid awkward small talk.[ 6 8 ] But in the collaborative workplace, constant use ofAirPods has sometimes been interpreted as a sign of disrespect to fellow co-workers, causing conflict in theoffice.[ 6 9 ]Another prominent criticism was a problem that caused the charging case battery to deplete at a rapid rate\n",
      "The question is: product's brief explanation, does not contain the word wikipedia\n",
      "Here the result of the semantic search on the index, without GPT4All..\n",
      "23. 5. 30. 오후  6:12 AirPods Max - W ikipediahttps://en.wikipedia.or g/wiki/AirPods_Max 1/6AirPods MaxAirPods Max in Space GrayDeveloper Apple Inc.ManufacturerLuxshare (on contract),Goertek (on contract)ProductfamilyAirPodsType Wireless headphonesRelease dateDecember 15, 2020IntroductorypriceUS$549.00System on achipApple H1 (each ear cup)Input Nine microphones, opticalsensor (each ear cup),position sensor (each earcup), case-detect sensor(each ear cup),accelerometer (each ear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  3827.46 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  5364.73 ms /    16 tokens (  335.30 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  5373.74 ms\n"
     ]
    }
   ],
   "source": [
    "pdf_folder_path = './docs'\n",
    "doc_list = [s for s in os.listdir(pdf_folder_path) if s.endswith('.pdf')]\n",
    "\n",
    "dic = {}\n",
    "for file_nm in doc_list:\n",
    "    file_nm = file_nm.split('.')[0]\n",
    "    index = FAISS.load_local(file_nm, embeddings)\n",
    "    # Load our local index vector db\n",
    "#     query = \"Product description\".format(file_nm)\n",
    "    query = \"product's brief explanation, does not contain the word wikipedia\"\n",
    "    docs = index.similarity_search(query)\n",
    "    # Get the matches best 3 results - defined in the function k=3\n",
    "    print(f\"The question is: {query}\")\n",
    "    print(\"Here the result of the semantic search on the index, without GPT4All..\")\n",
    "#     print(docs)\n",
    "    dic[file_nm] = {}\n",
    "    dic[file_nm]['information'] = ''.join(docs[1].page_content.split('\\n'))\n",
    "    print(dic[file_nm]['information'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f223aa98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T03:49:27.597572Z",
     "start_time": "2023-05-31T03:49:27.589116Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe format of the Context is in the order of \\'product name\\' \\'-\\' \\'description of the product\\'\\nAirPods - with each other. Having earphones in is generally perceived as a \"do not disturb\" signal, and some AirPods usersuse this fact to strategically avoid awkward small talk.[ 6 8 ] But in the collaborative workplace, constant use ofAirPods has sometimes been interpreted as a sign of disrespect to fellow co-workers, causing conflict in theoffice.[ 6 9 ]Another prominent criticism was a problem that caused the charging case battery to deplete at a rapid rate\\nThe format of the Context is in the order of \\'product name\\' \\'-\\' \\'description of the product\\'\\nAirPodsPro - accelerometers  and optical senso rs that can detec t presses on the stemand in-ear placement, and automati c pausing when they are taken out ofthe ears. Control by tapping is repla ced by pressing a force sensor on thestems.[ 7 ] They are rated IPX4  for water resistance.[ 3]The AirPods Pro use the H1 chip  also found in the second and third-generation AirPods, that supports hands-free \"Hey Siri\". They have activenoise cancellation , accomplished by microphones detecting outside sound\\nThe format of the Context is in the order of \\'product name\\' \\'-\\' \\'description of the product\\'\\nAirPodsMax - 23. 5. 30. 오후  6:12 AirPods Max - W ikipediahttps://en.wikipedia.or g/wiki/AirPods_Max 1/6AirPods MaxAirPods Max in Space GrayDeveloper Apple Inc.ManufacturerLuxshare (on contract),Goertek (on contract)ProductfamilyAirPodsType Wireless headphonesRelease dateDecember 15, 2020IntroductorypriceUS$549.00System on achipApple H1 (each ear cup)Input Nine microphones, opticalsensor (each ear cup),position sensor (each earcup), case-detect sensor(each ear cup),accelerometer (each ear\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "The format of the Context is in the order of 'product name' '-' 'description of the product'\n",
    "{} - {}\n",
    "The format of the Context is in the order of 'product name' '-' 'description of the product'\n",
    "{} - {}\n",
    "The format of the Context is in the order of 'product name' '-' 'description of the product'\n",
    "{} - {}\n",
    "\"\"\".format('AirPods',dic['AirPods']['information'], 'AirPodsPro',dic['AirPodsPro']['information'], 'AirPodsMax',dic['AirPodsMax']['information'])\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b172e700",
   "metadata": {},
   "source": [
    "# 질답\n",
    "- Template\n",
    "> - context : Faiss를 통해 뽑혀온 txt  \n",
    "> - question : 나의 질문  \n",
    "> - answer : LLM의 답변  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5227a8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:11:18.936325Z",
     "start_time": "2023-05-31T04:02:04.273762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please use the following context to answer questions.\n",
      "\n",
      "Context: \n",
      "The format of the Context is in the order of 'product name' '-' 'description of the product'\n",
      "AirPods - with each other. Having earphones in is generally perceived as a \"do not disturb\" signal, and some AirPods usersuse this fact to strategically avoid awkward small talk.[ 6 8 ] But in the collaborative workplace, constant use ofAirPods has sometimes been interpreted as a sign of disrespect to fellow co-workers, causing conflict in theoffice.[ 6 9 ]Another prominent criticism was a problem that caused the charging case battery to deplete at a rapid rate\n",
      "The format of the Context is in the order of 'product name' '-' 'description of the product'\n",
      "AirPodsPro - accelerometers  and optical senso rs that can detec t presses on the stemand in-ear placement, and automati c pausing when they are taken out ofthe ears. Control by tapping is repla ced by pressing a force sensor on thestems.[ 7 ] They are rated IPX4  for water resistance.[ 3]The AirPods Pro use the H1 chip  also found in the second and third-generation AirPods, that supports hands-free \"Hey Siri\". They have activenoise cancellation , accomplished by microphones detecting outside sound\n",
      "The format of the Context is in the order of 'product name' '-' 'description of the product'\n",
      "AirPodsMax - 23. 5. 30. 오후  6:12 AirPods Max - W ikipediahttps://en.wikipedia.or g/wiki/AirPods_Max 1/6AirPods MaxAirPods Max in Space GrayDeveloper Apple Inc.ManufacturerLuxshare (on contract),Goertek (on contract)ProductfamilyAirPodsType Wireless headphonesRelease dateDecember 15, 2020IntroductorypriceUS$549.00System on achipApple H1 (each ear cup)Input Nine microphones, opticalsensor (each ear cup),position sensor (each earcup), case-detect sensor(each ear cup),accelerometer (each ear\n",
      "\n",
      "---\n",
      "Question: Express airpods, airpods pro, airpods max product features like Instagram hashtags\n",
      "Answer: Let's think step by step. Hashtags on Instagram have the format of '#words' \u001b[0m\n",
      " / words without a dash between # and @ symbol/@ (example \"#AirPods Pro\" or \"hashtag\"). However, it is not necessary to make an airpohas tag in order for AirPodshashtags with that description.\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " / words without a dash between # and @ symbol/@ (example \"#AirPods Pro\" or \"hashtag\"). However, it is not necessary to make an airpohas tag in order for AirPodshashtags with that description.\n"
     ]
    }
   ],
   "source": [
    "# # Load our local index vector db\n",
    "# index = FAISS.load_local(\"my_faiss_index\", embeddings)\n",
    "\n",
    "# create the prompt template\n",
    "template = \"\"\"\n",
    "Please use the following context to answer questions.\n",
    "\n",
    "Context: {context}\n",
    "---\n",
    "Question: {question}\n",
    "Answer: Let's think step by step. Hashtags on Instagram have the format of '#words' \"\"\"\n",
    "\n",
    "# Hardcoded question\n",
    "question = \"Express airpods, airpods pro, airpods max product features like Instagram hashtags\"\n",
    "# instantiating the prompt template and the GPT4All chain\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"]).partial(context=context)\n",
    "handler = StdOutCallbackHandler()\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, callbacks=[handler])\n",
    "# Print the result\n",
    "print(llm_chain.run(question))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceec8462",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:20:47.053678Z",
     "start_time": "2023-05-31T04:11:18.939990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please use the following context to answer questions.\n",
      "\n",
      "Context: \n",
      "The format of the Context is in the order of 'product name' '-' 'description of the product'\n",
      "AirPods - with each other. Having earphones in is generally perceived as a \"do not disturb\" signal, and some AirPods usersuse this fact to strategically avoid awkward small talk.[ 6 8 ] But in the collaborative workplace, constant use ofAirPods has sometimes been interpreted as a sign of disrespect to fellow co-workers, causing conflict in theoffice.[ 6 9 ]Another prominent criticism was a problem that caused the charging case battery to deplete at a rapid rate\n",
      "The format of the Context is in the order of 'product name' '-' 'description of the product'\n",
      "AirPodsPro - accelerometers  and optical senso rs that can detec t presses on the stemand in-ear placement, and automati c pausing when they are taken out ofthe ears. Control by tapping is repla ced by pressing a force sensor on thestems.[ 7 ] They are rated IPX4  for water resistance.[ 3]The AirPods Pro use the H1 chip  also found in the second and third-generation AirPods, that supports hands-free \"Hey Siri\". They have activenoise cancellation , accomplished by microphones detecting outside sound\n",
      "The format of the Context is in the order of 'product name' '-' 'description of the product'\n",
      "AirPodsMax - 23. 5. 30. 오후  6:12 AirPods Max - W ikipediahttps://en.wikipedia.or g/wiki/AirPods_Max 1/6AirPods MaxAirPods Max in Space GrayDeveloper Apple Inc.ManufacturerLuxshare (on contract),Goertek (on contract)ProductfamilyAirPodsType Wireless headphonesRelease dateDecember 15, 2020IntroductorypriceUS$549.00System on achipApple H1 (each ear cup)Input Nine microphones, opticalsensor (each ear cup),position sensor (each earcup), case-detect sensor(each ear cup),accelerometer (each ear\n",
      "\n",
      "---\n",
      "Question: Express airpods, airpods pro, airpods max product features like Instagram hashtags\n",
      "Answer: Let's think step by step. \u001b[0m\n",
      " AirPODS - with each other and the same function to be used for both iPhone 11/11 Pro / 12Pro series as well in MacBookAir 13-2mm 2018 (Retina)Series 2 , 15\"MacBook, MacBook 2017(13\"), 17Inch Mac mini 5120GB SSD1TB 1.5GHz dual core Intel Core M YD1288XGZYU8M3K4A6GTI\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " AirPODS - with each other and the same function to be used for both iPhone 11/11 Pro / 12Pro series as well in MacBookAir 13-2mm 2018 (Retina)Series 2 , 15\"MacBook, MacBook 2017(13\"), 17Inch Mac mini 5120GB SSD1TB 1.5GHz dual core Intel Core M YD1288XGZYU8M3K4A6GTI\n"
     ]
    }
   ],
   "source": [
    "# # Load our local index vector db\n",
    "# index = FAISS.load_local(\"my_faiss_index\", embeddings)\n",
    "\n",
    "# create the prompt template\n",
    "template = \"\"\"\n",
    "Please use the following context to answer questions.\n",
    "\n",
    "Context: {context}\n",
    "---\n",
    "Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "# Hardcoded question\n",
    "question = \"Express airpods, airpods pro, airpods max product features like Instagram hashtags\"\n",
    "# instantiating the prompt template and the GPT4All chain\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"]).partial(context=context)\n",
    "handler = StdOutCallbackHandler()\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, callbacks=[handler])\n",
    "# Print the result\n",
    "print(llm_chain.run(question))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4839a040",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:29:21.542472Z",
     "start_time": "2023-05-31T04:20:47.056188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please use the following context to answer questions.\n",
      "\n",
      "Context: \n",
      "The format of the Context is in the order of 'product name' '-' 'description of the product'\n",
      "AirPods - with each other. Having earphones in is generally perceived as a \"do not disturb\" signal, and some AirPods usersuse this fact to strategically avoid awkward small talk.[ 6 8 ] But in the collaborative workplace, constant use ofAirPods has sometimes been interpreted as a sign of disrespect to fellow co-workers, causing conflict in theoffice.[ 6 9 ]Another prominent criticism was a problem that caused the charging case battery to deplete at a rapid rate\n",
      "The format of the Context is in the order of 'product name' '-' 'description of the product'\n",
      "AirPodsPro - accelerometers  and optical senso rs that can detec t presses on the stemand in-ear placement, and automati c pausing when they are taken out ofthe ears. Control by tapping is repla ced by pressing a force sensor on thestems.[ 7 ] They are rated IPX4  for water resistance.[ 3]The AirPods Pro use the H1 chip  also found in the second and third-generation AirPods, that supports hands-free \"Hey Siri\". They have activenoise cancellation , accomplished by microphones detecting outside sound\n",
      "The format of the Context is in the order of 'product name' '-' 'description of the product'\n",
      "AirPodsMax - 23. 5. 30. 오후  6:12 AirPods Max - W ikipediahttps://en.wikipedia.or g/wiki/AirPods_Max 1/6AirPods MaxAirPods Max in Space GrayDeveloper Apple Inc.ManufacturerLuxshare (on contract),Goertek (on contract)ProductfamilyAirPodsType Wireless headphonesRelease dateDecember 15, 2020IntroductorypriceUS$549.00System on achipApple H1 (each ear cup)Input Nine microphones, opticalsensor (each ear cup),position sensor (each earcup), case-detect sensor(each ear cup),accelerometer (each ear\n",
      "\n",
      "---\n",
      "Question: Express AirPods, AirPodsPro, AirPodsMax product features like Instagram hashtags\n",
      "Answer: Let's think step by step. \u001b[0m\n",
      " 1) The first generation of the Air Podes had a square-shaped case with no visible antennae and were only available in space gray color\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " 1) The first generation of the Air Podes had a square-shaped case with no visible antennae and were only available in space gray color\n"
     ]
    }
   ],
   "source": [
    "# # Load our local index vector db\n",
    "# index = FAISS.load_local(\"my_faiss_index\", embeddings)\n",
    "\n",
    "# create the prompt template\n",
    "template = \"\"\"\n",
    "Please use the following context to answer questions.\n",
    "\n",
    "Context: {context}\n",
    "---\n",
    "Question: {question}\n",
    "Answer: Let's think step by step. \"\"\"\n",
    "\n",
    "# Hardcoded question\n",
    "question = \"Express AirPods, AirPodsPro, AirPodsMax product features like Instagram hashtags\"\n",
    "# instantiating the prompt template and the GPT4All chain\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"]).partial(context=context)\n",
    "handler = StdOutCallbackHandler()\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, callbacks=[handler])\n",
    "# Print the result\n",
    "print(llm_chain.run(question))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4202c7a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:39:10.402973Z",
     "start_time": "2023-05-31T04:30:25.526511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please use the following context to answer questions.\n",
      "\n",
      "Context: \n",
      "The format of the Context is in the order of 'product name' '-' 'description of the product'\n",
      "AirPods - with each other. Having earphones in is generally perceived as a \"do not disturb\" signal, and some AirPods usersuse this fact to strategically avoid awkward small talk.[ 6 8 ] But in the collaborative workplace, constant use ofAirPods has sometimes been interpreted as a sign of disrespect to fellow co-workers, causing conflict in theoffice.[ 6 9 ]Another prominent criticism was a problem that caused the charging case battery to deplete at a rapid rate\n",
      "The format of the Context is in the order of 'product name' '-' 'description of the product'\n",
      "AirPodsPro - accelerometers  and optical senso rs that can detec t presses on the stemand in-ear placement, and automati c pausing when they are taken out ofthe ears. Control by tapping is repla ced by pressing a force sensor on thestems.[ 7 ] They are rated IPX4  for water resistance.[ 3]The AirPods Pro use the H1 chip  also found in the second and third-generation AirPods, that supports hands-free \"Hey Siri\". They have activenoise cancellation , accomplished by microphones detecting outside sound\n",
      "The format of the Context is in the order of 'product name' '-' 'description of the product'\n",
      "AirPodsMax - 23. 5. 30. 오후  6:12 AirPods Max - W ikipediahttps://en.wikipedia.or g/wiki/AirPods_Max 1/6AirPods MaxAirPods Max in Space GrayDeveloper Apple Inc.ManufacturerLuxshare (on contract),Goertek (on contract)ProductfamilyAirPodsType Wireless headphonesRelease dateDecember 15, 2020IntroductorypriceUS$549.00System on achipApple H1 (each ear cup)Input Nine microphones, opticalsensor (each ear cup),position sensor (each earcup), case-detect sensor(each ear cup),accelerometer (each ear\n",
      "\n",
      "---\n",
      "Question: \n",
      "Can you give me some simple keywords to describe the product below?\n",
      "Products are separated by ','. \n",
      "AirPods, AirPodsPro, AirPodsMax \n",
      "Answer: Let's think step by step. \u001b[0m\n",
      "- Can we come up with \"Earphones\" as a keyword for multiple products (i.e., one in the first product and another\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "- Can we come up with \"Earphones\" as a keyword for multiple products (i.e., one in the first product and another\n"
     ]
    }
   ],
   "source": [
    "# # Load our local index vector db\n",
    "# index = FAISS.load_local(\"my_faiss_index\", embeddings)\n",
    "\n",
    "# create the prompt template\n",
    "template = \"\"\"\n",
    "Please use the following context to answer questions.\n",
    "\n",
    "Context: {context}\n",
    "---\n",
    "Question: {question}\n",
    "Answer: Let's think step by step. \"\"\"\n",
    "\n",
    "# Hardcoded question\n",
    "question = \"\"\"\n",
    "Can you give me some simple keywords to describe the product below?\n",
    "Products are separated by ','. \n",
    "AirPods, AirPodsPro, AirPodsMax \"\"\"\n",
    "\n",
    "# instantiating the prompt template and the GPT4All chain\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"]).partial(context=context)\n",
    "handler = StdOutCallbackHandler()\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, callbacks=[handler])\n",
    "# Print the result\n",
    "print(llm_chain.run(question))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09bd47a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:54:06.399592Z",
     "start_time": "2023-05-31T04:41:45.261829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please use the following context to answer questions.\n",
      "\n",
      "Context: \n",
      "The format of the Context is in the order of 'product name' '-' 'description of the product'\n",
      "AirPods - with each other. Having earphones in is generally perceived as a \"do not disturb\" signal, and some AirPods usersuse this fact to strategically avoid awkward small talk.[ 6 8 ] But in the collaborative workplace, constant use ofAirPods has sometimes been interpreted as a sign of disrespect to fellow co-workers, causing conflict in theoffice.[ 6 9 ]Another prominent criticism was a problem that caused the charging case battery to deplete at a rapid rate\n",
      "The format of the Context is in the order of 'product name' '-' 'description of the product'\n",
      "AirPodsPro - accelerometers  and optical senso rs that can detec t presses on the stemand in-ear placement, and automati c pausing when they are taken out ofthe ears. Control by tapping is repla ced by pressing a force sensor on thestems.[ 7 ] They are rated IPX4  for water resistance.[ 3]The AirPods Pro use the H1 chip  also found in the second and third-generation AirPods, that supports hands-free \"Hey Siri\". They have activenoise cancellation , accomplished by microphones detecting outside sound\n",
      "The format of the Context is in the order of 'product name' '-' 'description of the product'\n",
      "AirPodsMax - 23. 5. 30. 오후  6:12 AirPods Max - W ikipediahttps://en.wikipedia.or g/wiki/AirPods_Max 1/6AirPods MaxAirPods Max in Space GrayDeveloper Apple Inc.ManufacturerLuxshare (on contract),Goertek (on contract)ProductfamilyAirPodsType Wireless headphonesRelease dateDecember 15, 2020IntroductorypriceUS$549.00System on achipApple H1 (each ear cup)Input Nine microphones, opticalsensor (each ear cup),position sensor (each earcup), case-detect sensor(each ear cup),accelerometer (each ear\n",
      "\n",
      "---\n",
      "Question: Explain the product description of AirPods, AirPods Pro, and AirPods Max with simple keywords\n",
      "Answer: Let's think step by step. \u001b[0m\n",
      "\n",
      " 1) The first thing to mention is about the original or classic version which was made in December 2020 but its price has been reduced because Apple did not expect people will buy it at that time so there were no plans of making a new product for this year, and then we have AirPods Pro with 23.5 mm ear-buds (sensor) + 3D audio technology - W i k e d ipedia http://en/wiki/Airp ods\\_Pro which cost USD$599 or Goorteck price on contract $549 without tax, and the last one is AirPods Max with 23.5 mm ear-buds (sensor) + 3D audio technology - W i k e d ipedia http://en/wiki /Airp ods\\_Max which has a system for space gray color that cost USD$599 or Goorteck price on contract $549 without tax, and the last one is Space Gray model with 23.5 mm ear-buds (sensor) + 3D audio technology - W i k e d ipedia http://en/wiki /Airp ods\\_Max \\_SpaceGray which cost USD$649 or Goorteck price on contract\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      " 1) The first thing to mention is about the original or classic version which was made in December 2020 but its price has been reduced because Apple did not expect people will buy it at that time so there were no plans of making a new product for this year, and then we have AirPods Pro with 23.5 mm ear-buds (sensor) + 3D audio technology - W i k e d ipedia http://en/wiki/Airp ods\\_Pro which cost USD$599 or Goorteck price on contract $549 without tax, and the last one is AirPods Max with 23.5 mm ear-buds (sensor) + 3D audio technology - W i k e d ipedia http://en/wiki /Airp ods\\_Max which has a system for space gray color that cost USD$599 or Goorteck price on contract $549 without tax, and the last one is Space Gray model with 23.5 mm ear-buds (sensor) + 3D audio technology - W i k e d ipedia http://en/wiki /Airp ods\\_Max \\_SpaceGray which cost USD$649 or Goorteck price on contract\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Load our local index vector db\n",
    "# index = FAISS.load_local(\"my_faiss_index\", embeddings)\n",
    "\n",
    "# create the prompt template\n",
    "template = \"\"\"\n",
    "Please use the following context to answer questions.\n",
    "\n",
    "Context: {context}\n",
    "---\n",
    "Question: {question}\n",
    "Answer: Let's think step by step. \"\"\"\n",
    "\n",
    "# Hardcoded question\n",
    "question = \"\"\"Explain the product description of AirPods, AirPods Pro, and AirPods Max with simple keywords\"\"\"\n",
    "\n",
    "# instantiating the prompt template and the GPT4All chain\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"]).partial(context=context)\n",
    "handler = StdOutCallbackHandler()\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, callbacks=[handler])\n",
    "# Print the result\n",
    "print(llm_chain.run(question))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c35fcc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:59:44.772600Z",
     "start_time": "2023-05-31T04:55:29.156384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "---\n",
      "Question: Explain the product description of AirPods, AirPods Pro, and AirPods Max with simple keywords\n",
      "Answer: Let's think step by step. \u001b[0m\n",
      "\n",
      " 1. Product name (Air Pods): The company that produces these earphones is Apple Inc., which sells a variety of products under its umbrella such as iPhones or iPads in the market segment for personal electronics devices like portable media players, computers and gaming systems. So to make it clear what this product stands for: \"EarPods\", meaning small electronic device with wires inside your ears that transmit sound (like music) from an outside source into them through a tiny opening on top of each earphone which then sends the signal wirelessly via Bluetooth 4 or later versions up and down to some sort of receiver, amplifier if needed.\" 2. Product description: \"EarPods\" are small electronic devices that fit comfortably in your ears with wires inside for sound transmission from an outside source through a tiny opening on top into each earphone which then sends the signal wirelessly via Bluetooth 4 or later versions up and down to some sort of receiver, amplifier if needed.\" 3. Product description: \"AirPods Pro\" - The product's name reflects its upgraded design over EarPonds that include longer battery life for wireless charging on compatible devices like iPhone 8/8s models with Lightning connector (also available in AirPower or Wireless Charging cases). It also\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      " 1. Product name (Air Pods): The company that produces these earphones is Apple Inc., which sells a variety of products under its umbrella such as iPhones or iPads in the market segment for personal electronics devices like portable media players, computers and gaming systems. So to make it clear what this product stands for: \"EarPods\", meaning small electronic device with wires inside your ears that transmit sound (like music) from an outside source into them through a tiny opening on top of each earphone which then sends the signal wirelessly via Bluetooth 4 or later versions up and down to some sort of receiver, amplifier if needed.\" 2. Product description: \"EarPods\" are small electronic devices that fit comfortably in your ears with wires inside for sound transmission from an outside source through a tiny opening on top into each earphone which then sends the signal wirelessly via Bluetooth 4 or later versions up and down to some sort of receiver, amplifier if needed.\" 3. Product description: \"AirPods Pro\" - The product's name reflects its upgraded design over EarPonds that include longer battery life for wireless charging on compatible devices like iPhone 8/8s models with Lightning connector (also available in AirPower or Wireless Charging cases). It also\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Load our local index vector db\n",
    "# index = FAISS.load_local(\"my_faiss_index\", embeddings)\n",
    "\n",
    "# create the prompt template\n",
    "template = \"\"\"\n",
    "---\n",
    "Question: {question}\n",
    "Answer: Let's think step by step. \"\"\"\n",
    "\n",
    "# Hardcoded question\n",
    "question = \"\"\"Explain the product description of AirPods, AirPods Pro, and AirPods Max with simple keywords\"\"\"\n",
    "\n",
    "# instantiating the prompt template and the GPT4All chain\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "handler = StdOutCallbackHandler()\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, callbacks=[handler])\n",
    "# Print the result\n",
    "print(llm_chain.run(question))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1936765",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7de7a30",
   "metadata": {},
   "source": [
    "여러 질문을 해보았지만 오히려 context를 제외한 답변이 제일 만족스러웟음.  \n",
    "\n",
    "context의 품질이 좋지 않았나 ? 더 많은 실험이 필요해보임.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
