{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f8927b5",
   "metadata": {},
   "source": [
    "# 목표 : spark DF DeepFM Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f17745",
   "metadata": {},
   "source": [
    "### 1. Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a7de25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-05 19:52:22.170355: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from modules.train import get_data\n",
    "from modules.DeepFM import DeepFM\n",
    "import modules.config as config\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, AUC\n",
    "import pandas as pd\n",
    "# data load\n",
    "test = pd.read_parquet('data/test.parquet')\n",
    "y_test = test['target']\n",
    "x_test = test.drop('target',axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ea7c06",
   "metadata": {},
   "source": [
    "### 2. model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83ea7b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Prepared...\n",
      "X shape: (32561, 108)\n",
      "# of Feature: 108\n",
      "# of Field: 14\n",
      "train/test save complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-05 19:52:28.391829: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# model load\n",
    "_, _, field_dict, field_index = get_data()\n",
    "fm = DeepFM(embedding_size=config.EMBEDDING_SIZE, num_feature=len(field_index),\n",
    "               num_field=len(field_dict), field_index=field_index)\n",
    "\n",
    "fm.build(input_shape = (1,len(field_index)))\n",
    "fm.load_weights('./weights/weights-epoch(10)-batch(256)-embedding(5).h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b8ddb",
   "metadata": {},
   "source": [
    "### 2. Pandas DF -> Spark DF convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3435070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: double (nullable = true)\n",
      " |-- workclass- ?: long (nullable = true)\n",
      " |-- workclass- Federal-gov: long (nullable = true)\n",
      " |-- workclass- Local-gov: long (nullable = true)\n",
      " |-- workclass- Never-worked: long (nullable = true)\n",
      " |-- workclass- Private: long (nullable = true)\n",
      " |-- workclass- Self-emp-inc: long (nullable = true)\n",
      " |-- workclass- Self-emp-not-inc: long (nullable = true)\n",
      " |-- workclass- State-gov: long (nullable = true)\n",
      " |-- workclass- Without-pay: long (nullable = true)\n",
      " |-- fnlwgt: double (nullable = true)\n",
      " |-- education- 10th: long (nullable = true)\n",
      " |-- education- 11th: long (nullable = true)\n",
      " |-- education- 12th: long (nullable = true)\n",
      " |-- education- 1st-4th: long (nullable = true)\n",
      " |-- education- 5th-6th: long (nullable = true)\n",
      " |-- education- 7th-8th: long (nullable = true)\n",
      " |-- education- 9th: long (nullable = true)\n",
      " |-- education- Assoc-acdm: long (nullable = true)\n",
      " |-- education- Assoc-voc: long (nullable = true)\n",
      " |-- education- Bachelors: long (nullable = true)\n",
      " |-- education- Doctorate: long (nullable = true)\n",
      " |-- education- HS-grad: long (nullable = true)\n",
      " |-- education- Masters: long (nullable = true)\n",
      " |-- education- Preschool: long (nullable = true)\n",
      " |-- education- Prof-school: long (nullable = true)\n",
      " |-- education- Some-college: long (nullable = true)\n",
      " |-- education-num: double (nullable = true)\n",
      " |-- marital-status- Divorced: long (nullable = true)\n",
      " |-- marital-status- Married-AF-spouse: long (nullable = true)\n",
      " |-- marital-status- Married-civ-spouse: long (nullable = true)\n",
      " |-- marital-status- Married-spouse-absent: long (nullable = true)\n",
      " |-- marital-status- Never-married: long (nullable = true)\n",
      " |-- marital-status- Separated: long (nullable = true)\n",
      " |-- marital-status- Widowed: long (nullable = true)\n",
      " |-- occupation- ?: long (nullable = true)\n",
      " |-- occupation- Adm-clerical: long (nullable = true)\n",
      " |-- occupation- Armed-Forces: long (nullable = true)\n",
      " |-- occupation- Craft-repair: long (nullable = true)\n",
      " |-- occupation- Exec-managerial: long (nullable = true)\n",
      " |-- occupation- Farming-fishing: long (nullable = true)\n",
      " |-- occupation- Handlers-cleaners: long (nullable = true)\n",
      " |-- occupation- Machine-op-inspct: long (nullable = true)\n",
      " |-- occupation- Other-service: long (nullable = true)\n",
      " |-- occupation- Priv-house-serv: long (nullable = true)\n",
      " |-- occupation- Prof-specialty: long (nullable = true)\n",
      " |-- occupation- Protective-serv: long (nullable = true)\n",
      " |-- occupation- Sales: long (nullable = true)\n",
      " |-- occupation- Tech-support: long (nullable = true)\n",
      " |-- occupation- Transport-moving: long (nullable = true)\n",
      " |-- relationship- Husband: long (nullable = true)\n",
      " |-- relationship- Not-in-family: long (nullable = true)\n",
      " |-- relationship- Other-relative: long (nullable = true)\n",
      " |-- relationship- Own-child: long (nullable = true)\n",
      " |-- relationship- Unmarried: long (nullable = true)\n",
      " |-- relationship- Wife: long (nullable = true)\n",
      " |-- race- Amer-Indian-Eskimo: long (nullable = true)\n",
      " |-- race- Asian-Pac-Islander: long (nullable = true)\n",
      " |-- race- Black: long (nullable = true)\n",
      " |-- race- Other: long (nullable = true)\n",
      " |-- race- White: long (nullable = true)\n",
      " |-- sex- Female: long (nullable = true)\n",
      " |-- sex- Male: long (nullable = true)\n",
      " |-- capital-gain: double (nullable = true)\n",
      " |-- capital-loss: double (nullable = true)\n",
      " |-- hours-per-week: double (nullable = true)\n",
      " |-- country- ?: long (nullable = true)\n",
      " |-- country- Cambodia: long (nullable = true)\n",
      " |-- country- Canada: long (nullable = true)\n",
      " |-- country- China: long (nullable = true)\n",
      " |-- country- Columbia: long (nullable = true)\n",
      " |-- country- Cuba: long (nullable = true)\n",
      " |-- country- Dominican-Republic: long (nullable = true)\n",
      " |-- country- Ecuador: long (nullable = true)\n",
      " |-- country- El-Salvador: long (nullable = true)\n",
      " |-- country- England: long (nullable = true)\n",
      " |-- country- France: long (nullable = true)\n",
      " |-- country- Germany: long (nullable = true)\n",
      " |-- country- Greece: long (nullable = true)\n",
      " |-- country- Guatemala: long (nullable = true)\n",
      " |-- country- Haiti: long (nullable = true)\n",
      " |-- country- Holand-Netherlands: long (nullable = true)\n",
      " |-- country- Honduras: long (nullable = true)\n",
      " |-- country- Hong: long (nullable = true)\n",
      " |-- country- Hungary: long (nullable = true)\n",
      " |-- country- India: long (nullable = true)\n",
      " |-- country- Iran: long (nullable = true)\n",
      " |-- country- Ireland: long (nullable = true)\n",
      " |-- country- Italy: long (nullable = true)\n",
      " |-- country- Jamaica: long (nullable = true)\n",
      " |-- country- Japan: long (nullable = true)\n",
      " |-- country- Laos: long (nullable = true)\n",
      " |-- country- Mexico: long (nullable = true)\n",
      " |-- country- Nicaragua: long (nullable = true)\n",
      " |-- country- Outlying-US(Guam-USVI-etc): long (nullable = true)\n",
      " |-- country- Peru: long (nullable = true)\n",
      " |-- country- Philippines: long (nullable = true)\n",
      " |-- country- Poland: long (nullable = true)\n",
      " |-- country- Portugal: long (nullable = true)\n",
      " |-- country- Puerto-Rico: long (nullable = true)\n",
      " |-- country- Scotland: long (nullable = true)\n",
      " |-- country- South: long (nullable = true)\n",
      " |-- country- Taiwan: long (nullable = true)\n",
      " |-- country- Thailand: long (nullable = true)\n",
      " |-- country- Trinadad&Tobago: long (nullable = true)\n",
      " |-- country- United-States: long (nullable = true)\n",
      " |-- country- Vietnam: long (nullable = true)\n",
      " |-- country- Yugoslavia: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#Create PySpark SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()\n",
    "#Create PySpark DataFrame from Pandas\n",
    "sparkDF=spark.createDataFrame(x_test) \n",
    "sparkDF.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e035b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark.sql.functions as F\n",
    "# from pyspark.ml.feature import OneHotEncoder\n",
    "# from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# indexer = StringIndexer(inputCols=CAT_FIELDS, outputCols=[col + \"_encoded\" for col in CAT_FIELDS])\n",
    "# label_df = indexer.fit(sparkDF).transform(sparkDF)\n",
    "# label_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e28fb29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0488150417804718"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = float(fm(x_test.head(1).values))\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3987d9",
   "metadata": {},
   "source": [
    "### UDF predit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "128cc5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# column order required by the model.\n",
    "FEATURES = x_test.columns\n",
    "path = './weights/weights-epoch(10)-batch(256)-embedding(5).h5' \n",
    "\n",
    "def predict(features):\n",
    "\n",
    "    np_features = np.array([features])\n",
    "    \n",
    "    \n",
    "#     # model load\n",
    "#     fm = DeepFM(embedding_size=config.EMBEDDING_SIZE, num_feature=len(field_index),\n",
    "#                num_field=len(field_dict), field_index=field_index)\n",
    "\n",
    "#     fm.build(input_shape = (1,len(field_index)))\n",
    "#     fm.load_weights(path)\n",
    "    \n",
    "    y = fm(np_features)\n",
    "\n",
    "\n",
    "    return float(y)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    " \n",
    "\n",
    "predict_udf = udf(predict, DoubleType())\n",
    "\n",
    "\n",
    "# test용 udf\n",
    "def test(x):\n",
    "    return x \n",
    "test_udf = udf(test, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e670b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparkDF2 = sparkDF.withColumn(\n",
    "#     \"test\",\n",
    "#     test_udf(F.col('country- Philippines'))\n",
    "# )\n",
    "# sparkDF2.select(F.col('test')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd97d95",
   "metadata": {},
   "source": [
    "### MapPartitions predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4ac486a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# column order required by the model.\n",
    "FEATURES = x_test.columns\n",
    "path = './weights/weights-epoch(10)-batch(256)-embedding(5).h5' \n",
    "\n",
    "def predict_partition(rows):\n",
    "    # model load\n",
    "    fm = DeepFM(embedding_size=config.EMBEDDING_SIZE, num_feature=len(field_index),\n",
    "               num_field=len(field_dict), field_index=field_index)\n",
    "    fm.build(input_shape = (1,len(field_index)))\n",
    "    fm.load_weights(path)\n",
    "    \n",
    "    y = fm(rows[FEATURES].values)\n",
    "    return y\n",
    "\n",
    "#     y = fm(rows[FEATURES].values)\n",
    "#     return float(y)\n",
    "\n",
    "\n",
    "def test(rows):\n",
    "    for row in rows:\n",
    "        yield row.age + row['workclass- Federal-gov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0b70944f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1917808219178082,\n",
       " 0.4520547945205479,\n",
       " 0.30136986301369856,\n",
       " 0.27397260273972607,\n",
       " 0.0,\n",
       " 0.1780821917808219,\n",
       " 0.6438356164383561,\n",
       " 0.2328767123287671,\n",
       " 0.46575342465753417,\n",
       " 0.1780821917808219,\n",
       " 0.1780821917808219,\n",
       " 0.2054794520547945,\n",
       " 0.2465753424657534,\n",
       " 0.35616438356164376,\n",
       " 0.1095890410958904,\n",
       " 0.0547945205479452,\n",
       " 1.191780821917808,\n",
       " 0.0547945205479452,\n",
       " 0.2465753424657534,\n",
       " 0.5616438356164384,\n",
       " 0.3150684931506849,\n",
       " 0.2328767123287671,\n",
       " 0.0410958904109589,\n",
       " 0.27397260273972607,\n",
       " 0.35616438356164376,\n",
       " 0.0273972602739726,\n",
       " 0.7123287671232876,\n",
       " 0.6164383561643836,\n",
       " 0.5616438356164384,\n",
       " 0.4931506849315069,\n",
       " 0.3424657534246575,\n",
       " 0.136986301369863,\n",
       " 0.3835616438356165,\n",
       " 0.0,\n",
       " 0.2054794520547945,\n",
       " 0.41095890410958896,\n",
       " 0.35616438356164376,\n",
       " 0.3424657534246575,\n",
       " 0.1232876712328767,\n",
       " 0.2602739726027397,\n",
       " 0.1506849315068493,\n",
       " 0.3150684931506849,\n",
       " 0.3698630136986301,\n",
       " 0.5753424657534245,\n",
       " 0.0821917808219178,\n",
       " 0.46575342465753417,\n",
       " 0.2602739726027397,\n",
       " 0.0958904109589041,\n",
       " 0.0136986301369863,\n",
       " 0.6575342465753424,\n",
       " 0.1643835616438356,\n",
       " 0.2465753424657534,\n",
       " 0.5068493150684932,\n",
       " 0.2191780821917808,\n",
       " 0.2602739726027397,\n",
       " 0.0821917808219178,\n",
       " 0.5205479452054793,\n",
       " 0.547945205479452,\n",
       " 0.1917808219178082,\n",
       " 0.5753424657534245,\n",
       " 0.6986301369863013,\n",
       " 0.4931506849315069,\n",
       " 0.2465753424657534,\n",
       " 0.6575342465753424,\n",
       " 0.3835616438356165,\n",
       " 0.5205479452054793,\n",
       " 0.2465753424657534,\n",
       " 0.136986301369863,\n",
       " 0.4931506849315069,\n",
       " 1.2602739726027397,\n",
       " 0.547945205479452,\n",
       " 0.3150684931506849,\n",
       " 0.5616438356164384,\n",
       " 0.4246575342465753,\n",
       " 0.32876712328767127,\n",
       " 0.2191780821917808,\n",
       " 0.2465753424657534,\n",
       " 0.0136986301369863,\n",
       " 0.2876712328767123,\n",
       " 0.6986301369863013,\n",
       " 0.30136986301369856,\n",
       " 0.2191780821917808,\n",
       " 0.3150684931506849,\n",
       " 0.35616438356164376,\n",
       " 0.8767123287671232,\n",
       " 0.6986301369863013,\n",
       " 0.0684931506849315,\n",
       " 0.0821917808219178,\n",
       " 0.1917808219178082,\n",
       " 0.41095890410958896,\n",
       " 0.2876712328767123,\n",
       " 0.2054794520547945,\n",
       " 0.7123287671232876,\n",
       " 0.3698630136986301,\n",
       " 0.4383561643835617,\n",
       " 0.0958904109589041,\n",
       " 0.3150684931506849,\n",
       " 0.0958904109589041,\n",
       " 0.0684931506849315,\n",
       " 0.0410958904109589,\n",
       " 0.4383561643835617,\n",
       " 0.3835616438356165,\n",
       " 0.3698630136986301,\n",
       " 0.32876712328767127,\n",
       " 0.0821917808219178,\n",
       " 0.6027397260273972,\n",
       " 0.3150684931506849,\n",
       " 0.32876712328767127,\n",
       " 0.6575342465753424,\n",
       " 0.3150684931506849,\n",
       " 0.0410958904109589,\n",
       " 0.32876712328767127,\n",
       " 0.5205479452054793,\n",
       " 0.1232876712328767,\n",
       " 0.1917808219178082,\n",
       " 0.6575342465753424,\n",
       " 0.1780821917808219,\n",
       " 0.46575342465753417,\n",
       " 0.3424657534246575,\n",
       " 0.0547945205479452,\n",
       " 1.36986301369863,\n",
       " 0.1917808219178082,\n",
       " 0.7808219178082192,\n",
       " 0.0547945205479452,\n",
       " 0.30136986301369856,\n",
       " 0.547945205479452,\n",
       " 0.0958904109589041,\n",
       " 0.5205479452054793,\n",
       " 0.3835616438356165,\n",
       " 0.4383561643835617,\n",
       " 0.27397260273972607,\n",
       " 0.32876712328767127,\n",
       " 0.0273972602739726,\n",
       " 0.0136986301369863,\n",
       " 0.27397260273972607,\n",
       " 0.0273972602739726,\n",
       " 0.0410958904109589,\n",
       " 0.0958904109589041,\n",
       " 0.1917808219178082,\n",
       " 0.2191780821917808,\n",
       " 0.3150684931506849,\n",
       " 0.5616438356164384,\n",
       " 0.0273972602739726,\n",
       " 0.136986301369863,\n",
       " 0.1780821917808219,\n",
       " 0.27397260273972607,\n",
       " 0.32876712328767127,\n",
       " 0.4246575342465753,\n",
       " 0.2328767123287671,\n",
       " 0.32876712328767127,\n",
       " 0.4931506849315069,\n",
       " 0.3424657534246575,\n",
       " 0.4931506849315069,\n",
       " 0.4520547945205479,\n",
       " 0.2876712328767123,\n",
       " 0.0821917808219178,\n",
       " 0.4520547945205479,\n",
       " 0.1095890410958904,\n",
       " 0.2054794520547945,\n",
       " 0.547945205479452,\n",
       " 0.0684931506849315,\n",
       " 1.2876712328767124,\n",
       " 0.2602739726027397,\n",
       " 0.0273972602739726,\n",
       " 0.4520547945205479,\n",
       " 0.4246575342465753,\n",
       " 0.136986301369863,\n",
       " 1.547945205479452,\n",
       " 0.1095890410958904,\n",
       " 0.35616438356164376,\n",
       " 0.3150684931506849,\n",
       " 0.136986301369863,\n",
       " 0.27397260273972607,\n",
       " 0.4246575342465753,\n",
       " 0.1780821917808219,\n",
       " 0.2602739726027397,\n",
       " 0.3972602739726027,\n",
       " 0.5342465753424657,\n",
       " 0.7397260273972601,\n",
       " 0.1095890410958904,\n",
       " 0.1917808219178082,\n",
       " 1.5068493150684932,\n",
       " 0.1917808219178082,\n",
       " 0.2191780821917808,\n",
       " 0.547945205479452,\n",
       " 0.0821917808219178,\n",
       " 0.4931506849315069,\n",
       " 0.2876712328767123,\n",
       " 0.5753424657534245,\n",
       " 0.1506849315068493,\n",
       " 0.0821917808219178,\n",
       " 0.32876712328767127,\n",
       " 1.5205479452054793,\n",
       " 0.1095890410958904,\n",
       " 0.5068493150684932,\n",
       " 1.178082191780822,\n",
       " 0.3424657534246575,\n",
       " 0.5890410958904109,\n",
       " 0.1643835616438356,\n",
       " 0.4246575342465753,\n",
       " 0.6438356164383561,\n",
       " 0.1232876712328767,\n",
       " 0.27397260273972607,\n",
       " 0.30136986301369856,\n",
       " 0.0,\n",
       " 0.7397260273972601,\n",
       " 0.3424657534246575,\n",
       " 0.1095890410958904,\n",
       " 0.5890410958904109,\n",
       " 0.2602739726027397,\n",
       " 0.2328767123287671,\n",
       " 0.2054794520547945,\n",
       " 0.6301369863013697,\n",
       " 0.5342465753424657,\n",
       " 0.0410958904109589,\n",
       " 0.6301369863013697,\n",
       " 0.3698630136986301,\n",
       " 0.0958904109589041,\n",
       " 0.1232876712328767,\n",
       " 0.0958904109589041,\n",
       " 0.1232876712328767,\n",
       " 0.0821917808219178,\n",
       " 0.2602739726027397,\n",
       " 0.2191780821917808,\n",
       " 0.2328767123287671,\n",
       " 0.0958904109589041,\n",
       " 0.0,\n",
       " 1.5342465753424657,\n",
       " 0.0684931506849315,\n",
       " 0.5753424657534245,\n",
       " 0.1095890410958904,\n",
       " 0.3835616438356165,\n",
       " 0.4520547945205479,\n",
       " 0.136986301369863,\n",
       " 0.1643835616438356,\n",
       " 0.3835616438356165,\n",
       " 0.1917808219178082,\n",
       " 0.5890410958904109,\n",
       " 0.0821917808219178,\n",
       " 0.2876712328767123,\n",
       " 0.6712328767123288,\n",
       " 0.0136986301369863,\n",
       " 0.3698630136986301,\n",
       " 0.1506849315068493,\n",
       " 0.2191780821917808,\n",
       " 0.1917808219178082,\n",
       " 0.2054794520547945,\n",
       " 0.1643835616438356,\n",
       " 0.46575342465753417,\n",
       " 0.1232876712328767,\n",
       " 0.41095890410958896,\n",
       " 0.6301369863013697,\n",
       " 0.3424657534246575,\n",
       " 0.2465753424657534,\n",
       " 0.2602739726027397,\n",
       " 0.1917808219178082,\n",
       " 0.2054794520547945,\n",
       " 0.4246575342465753,\n",
       " 0.7945205479452053,\n",
       " 0.1506849315068493,\n",
       " 0.27397260273972607,\n",
       " 0.6164383561643836,\n",
       " 0.27397260273972607,\n",
       " 0.2465753424657534,\n",
       " 0.2054794520547945,\n",
       " 0.0,\n",
       " 0.0958904109589041,\n",
       " 0.6712328767123288,\n",
       " 0.2191780821917808,\n",
       " 0.547945205479452,\n",
       " 0.1506849315068493,\n",
       " 0.7397260273972601,\n",
       " 0.5616438356164384,\n",
       " 0.1917808219178082,\n",
       " 0.1506849315068493,\n",
       " 0.2054794520547945,\n",
       " 0.3424657534246575,\n",
       " 0.2054794520547945,\n",
       " 0.2465753424657534,\n",
       " 0.3972602739726027,\n",
       " 0.4794520547945205,\n",
       " 0.4246575342465753,\n",
       " 0.1780821917808219,\n",
       " 0.0547945205479452,\n",
       " 0.35616438356164376,\n",
       " 0.3972602739726027,\n",
       " 0.0410958904109589,\n",
       " 0.0,\n",
       " 0.0410958904109589,\n",
       " 0.3835616438356165,\n",
       " 0.0958904109589041,\n",
       " 0.3424657534246575,\n",
       " 0.3835616438356165,\n",
       " 0.0273972602739726,\n",
       " 0.0684931506849315,\n",
       " 0.0,\n",
       " 0.136986301369863,\n",
       " 0.2602739726027397,\n",
       " 0.3835616438356165,\n",
       " 0.3698630136986301,\n",
       " 0.2328767123287671,\n",
       " 0.2876712328767123,\n",
       " 0.1643835616438356,\n",
       " 0.4246575342465753,\n",
       " 0.3150684931506849,\n",
       " 0.1232876712328767,\n",
       " 0.2465753424657534,\n",
       " 0.4246575342465753,\n",
       " 0.2465753424657534,\n",
       " 0.2465753424657534,\n",
       " 0.41095890410958896,\n",
       " 0.3835616438356165,\n",
       " 0.0684931506849315,\n",
       " 0.2054794520547945,\n",
       " 0.0136986301369863,\n",
       " 0.4520547945205479,\n",
       " 0.2328767123287671,\n",
       " 0.0684931506849315,\n",
       " 0.35616438356164376,\n",
       " 0.4383561643835617,\n",
       " 0.3698630136986301,\n",
       " 0.2602739726027397,\n",
       " 0.5205479452054793,\n",
       " 0.3150684931506849,\n",
       " 0.547945205479452,\n",
       " 0.0273972602739726,\n",
       " 0.5068493150684932,\n",
       " 0.1095890410958904,\n",
       " 0.0547945205479452,\n",
       " 0.3150684931506849,\n",
       " 0.2602739726027397,\n",
       " 0.1232876712328767,\n",
       " 0.2602739726027397,\n",
       " 0.1780821917808219,\n",
       " 0.4246575342465753,\n",
       " 0.4520547945205479,\n",
       " 0.0136986301369863,\n",
       " 0.30136986301369856,\n",
       " 0.4383561643835617,\n",
       " 0.1506849315068493,\n",
       " 0.8082191780821917,\n",
       " 0.1506849315068493,\n",
       " 0.1095890410958904,\n",
       " 0.5616438356164384,\n",
       " 0.27397260273972607,\n",
       " 0.5068493150684932,\n",
       " 0.2602739726027397,\n",
       " 0.0136986301369863,\n",
       " 0.35616438356164376,\n",
       " 0.3835616438356165,\n",
       " 0.7808219178082192,\n",
       " 0.2602739726027397,\n",
       " 0.2602739726027397,\n",
       " 0.1506849315068493,\n",
       " 0.0547945205479452,\n",
       " 0.0410958904109589,\n",
       " 0.4246575342465753,\n",
       " 0.30136986301369856,\n",
       " 0.2328767123287671,\n",
       " 0.0684931506849315,\n",
       " 0.4794520547945205,\n",
       " 0.4246575342465753,\n",
       " 0.1232876712328767,\n",
       " 0.5890410958904109,\n",
       " 0.0958904109589041,\n",
       " 0.2602739726027397,\n",
       " 0.6575342465753424,\n",
       " 0.3835616438356165,\n",
       " 0.1643835616438356,\n",
       " 0.2876712328767123,\n",
       " 0.0273972602739726,\n",
       " 0.0410958904109589,\n",
       " 0.30136986301369856,\n",
       " 0.3835616438356165,\n",
       " 0.4931506849315069,\n",
       " 0.1506849315068493,\n",
       " 0.4246575342465753,\n",
       " 0.547945205479452,\n",
       " 0.0821917808219178,\n",
       " 0.0136986301369863,\n",
       " 0.0410958904109589,\n",
       " 0.3698630136986301,\n",
       " 0.4246575342465753,\n",
       " 0.3835616438356165,\n",
       " 0.3150684931506849,\n",
       " 0.2465753424657534,\n",
       " 0.1232876712328767,\n",
       " 0.3835616438356165,\n",
       " 0.4383561643835617,\n",
       " 0.0684931506849315,\n",
       " 0.32876712328767127,\n",
       " 0.3972602739726027,\n",
       " 0.41095890410958896,\n",
       " 0.4520547945205479,\n",
       " 0.1643835616438356,\n",
       " 0.5205479452054793,\n",
       " 1.4931506849315068,\n",
       " 0.4794520547945205,\n",
       " 0.3698630136986301,\n",
       " 0.0273972602739726,\n",
       " 0.4246575342465753,\n",
       " 0.4931506849315069,\n",
       " 0.1232876712328767,\n",
       " 0.4246575342465753,\n",
       " 0.3424657534246575,\n",
       " 0.2602739726027397,\n",
       " 0.0821917808219178,\n",
       " 0.41095890410958896,\n",
       " 0.27397260273972607,\n",
       " 0.1917808219178082,\n",
       " 0.3835616438356165,\n",
       " 0.5068493150684932,\n",
       " 0.5753424657534245,\n",
       " 0.5205479452054793,\n",
       " 0.1643835616438356,\n",
       " 0.0684931506849315,\n",
       " 0.1643835616438356,\n",
       " 0.32876712328767127,\n",
       " 0.2054794520547945,\n",
       " 0.2602739726027397,\n",
       " 0.1917808219178082,\n",
       " 0.821917808219178,\n",
       " 0.2054794520547945,\n",
       " 0.1780821917808219,\n",
       " 0.1232876712328767,\n",
       " 0.1232876712328767,\n",
       " 0.0136986301369863,\n",
       " 0.35616438356164376,\n",
       " 0.1506849315068493,\n",
       " 0.0821917808219178,\n",
       " 0.0273972602739726,\n",
       " 0.1232876712328767,\n",
       " 0.30136986301369856,\n",
       " 0.46575342465753417,\n",
       " 0.1095890410958904,\n",
       " 0.2191780821917808,\n",
       " 0.5753424657534245,\n",
       " 0.0821917808219178,\n",
       " 0.30136986301369856,\n",
       " 0.0273972602739726,\n",
       " 0.1917808219178082,\n",
       " 0.2328767123287671,\n",
       " 0.3835616438356165,\n",
       " 0.4931506849315069,\n",
       " 1.36986301369863,\n",
       " 0.2054794520547945,\n",
       " 0.4383561643835617,\n",
       " 0.4246575342465753,\n",
       " 0.136986301369863,\n",
       " 0.1506849315068493,\n",
       " 0.0136986301369863,\n",
       " 0.1506849315068493,\n",
       " 0.3835616438356165,\n",
       " 0.4794520547945205,\n",
       " 0.0958904109589041,\n",
       " 0.4520547945205479,\n",
       " 0.4383561643835617,\n",
       " 0.1780821917808219,\n",
       " 0.0547945205479452,\n",
       " 0.2602739726027397,\n",
       " 1.410958904109589,\n",
       " 0.27397260273972607,\n",
       " 0.2876712328767123,\n",
       " 0.0958904109589041,\n",
       " 0.1643835616438356,\n",
       " 0.2054794520547945,\n",
       " 0.1506849315068493,\n",
       " 0.3150684931506849,\n",
       " 0.6438356164383561,\n",
       " 0.0821917808219178,\n",
       " 0.1643835616438356,\n",
       " 0.0684931506849315,\n",
       " 0.2191780821917808,\n",
       " 0.5616438356164384,\n",
       " 0.6712328767123288,\n",
       " 0.0821917808219178,\n",
       " 0.4520547945205479,\n",
       " 1.1643835616438356,\n",
       " 0.0547945205479452,\n",
       " 0.2191780821917808,\n",
       " 0.3972602739726027,\n",
       " 0.0410958904109589,\n",
       " 0.0136986301369863,\n",
       " 0.30136986301369856,\n",
       " 0.27397260273972607,\n",
       " 0.2602739726027397,\n",
       " 0.0821917808219178,\n",
       " 0.5068493150684932,\n",
       " 0.0410958904109589,\n",
       " 0.1917808219178082,\n",
       " 1.36986301369863,\n",
       " 0.3835616438356165,\n",
       " 1.3972602739726028,\n",
       " 0.46575342465753417,\n",
       " 0.4520547945205479,\n",
       " 0.32876712328767127,\n",
       " 0.4520547945205479,\n",
       " 0.0136986301369863,\n",
       " 0.0684931506849315,\n",
       " 0.0273972602739726,\n",
       " 1.3287671232876712,\n",
       " 0.1232876712328767,\n",
       " 0.41095890410958896,\n",
       " 0.1506849315068493,\n",
       " 0.4383561643835617,\n",
       " 0.4794520547945205,\n",
       " 0.1643835616438356,\n",
       " 0.2328767123287671,\n",
       " 0.6575342465753424,\n",
       " 0.27397260273972607,\n",
       " 0.3424657534246575,\n",
       " 0.5205479452054793,\n",
       " 0.4246575342465753,\n",
       " 0.1917808219178082,\n",
       " 0.1780821917808219,\n",
       " 0.2876712328767123,\n",
       " 0.2191780821917808,\n",
       " 0.0136986301369863,\n",
       " 0.46575342465753417,\n",
       " 0.6986301369863013,\n",
       " 0.0821917808219178,\n",
       " 0.41095890410958896,\n",
       " 0.0547945205479452,\n",
       " 0.1232876712328767,\n",
       " 0.4246575342465753,\n",
       " 0.46575342465753417,\n",
       " 0.1917808219178082,\n",
       " 0.2602739726027397,\n",
       " 0.3424657534246575,\n",
       " 0.0136986301369863,\n",
       " 0.1780821917808219,\n",
       " 0.27397260273972607,\n",
       " 0.46575342465753417,\n",
       " 0.1643835616438356,\n",
       " 0.5753424657534245,\n",
       " 0.2054794520547945,\n",
       " 0.0136986301369863,\n",
       " 0.0,\n",
       " 0.1780821917808219,\n",
       " 0.35616438356164376,\n",
       " 0.2191780821917808,\n",
       " 0.35616438356164376,\n",
       " 0.1506849315068493,\n",
       " 0.0547945205479452,\n",
       " 0.2328767123287671,\n",
       " 0.2054794520547945,\n",
       " 0.46575342465753417,\n",
       " 0.3424657534246575,\n",
       " 0.30136986301369856,\n",
       " 0.2328767123287671,\n",
       " 0.2465753424657534,\n",
       " 0.6027397260273972,\n",
       " 0.5753424657534245,\n",
       " 0.1506849315068493,\n",
       " 0.3424657534246575,\n",
       " 0.1506849315068493,\n",
       " 0.1643835616438356,\n",
       " 0.7945205479452053,\n",
       " 0.4520547945205479,\n",
       " 0.3698630136986301,\n",
       " 0.547945205479452,\n",
       " 0.136986301369863,\n",
       " 0.136986301369863,\n",
       " 0.27397260273972607,\n",
       " 0.0410958904109589,\n",
       " 0.2465753424657534,\n",
       " 0.1780821917808219,\n",
       " 0.1917808219178082,\n",
       " 0.2054794520547945,\n",
       " 0.2054794520547945,\n",
       " 0.41095890410958896,\n",
       " 0.3150684931506849,\n",
       " 0.3698630136986301,\n",
       " 0.2876712328767123,\n",
       " 0.32876712328767127,\n",
       " 0.0821917808219178,\n",
       " 0.3150684931506849,\n",
       " 0.2876712328767123,\n",
       " 0.0136986301369863,\n",
       " 0.5342465753424657,\n",
       " 0.3698630136986301,\n",
       " 0.0410958904109589,\n",
       " 0.1232876712328767,\n",
       " 0.2191780821917808,\n",
       " 0.0,\n",
       " 0.1780821917808219,\n",
       " 0.30136986301369856,\n",
       " 0.3150684931506849,\n",
       " 0.30136986301369856,\n",
       " 0.136986301369863,\n",
       " 0.1917808219178082,\n",
       " 0.5205479452054793,\n",
       " 0.547945205479452,\n",
       " 0.2876712328767123,\n",
       " 0.1643835616438356,\n",
       " 0.2328767123287671,\n",
       " 0.1917808219178082,\n",
       " 0.5890410958904109,\n",
       " 0.7808219178082192,\n",
       " 0.27397260273972607,\n",
       " 0.5753424657534245,\n",
       " 0.1095890410958904,\n",
       " 0.4794520547945205,\n",
       " 0.32876712328767127,\n",
       " 0.3698630136986301,\n",
       " 0.0,\n",
       " 0.3972602739726027,\n",
       " 0.4931506849315069,\n",
       " 0.1917808219178082,\n",
       " 0.1917808219178082,\n",
       " 0.6164383561643836,\n",
       " 0.30136986301369856,\n",
       " 0.0410958904109589,\n",
       " 0.0547945205479452,\n",
       " 0.2876712328767123,\n",
       " 0.0547945205479452,\n",
       " 0.2876712328767123,\n",
       " 0.27397260273972607,\n",
       " 0.1095890410958904,\n",
       " 0.547945205479452,\n",
       " 0.3835616438356165,\n",
       " 0.136986301369863,\n",
       " 0.547945205479452,\n",
       " 0.3972602739726027,\n",
       " 0.1095890410958904,\n",
       " 0.4246575342465753,\n",
       " 0.1780821917808219,\n",
       " 0.35616438356164376,\n",
       " 0.2328767123287671,\n",
       " 0.3150684931506849,\n",
       " 0.0136986301369863,\n",
       " 0.5890410958904109,\n",
       " 0.35616438356164376,\n",
       " 0.2328767123287671,\n",
       " 0.0821917808219178,\n",
       " 0.0684931506849315,\n",
       " 0.6027397260273972,\n",
       " 0.3972602739726027,\n",
       " 0.0821917808219178,\n",
       " 0.4794520547945205,\n",
       " 0.0958904109589041,\n",
       " 0.1643835616438356,\n",
       " 0.46575342465753417,\n",
       " 0.1917808219178082,\n",
       " 0.5616438356164384,\n",
       " 1.2602739726027397,\n",
       " 0.2465753424657534,\n",
       " 0.2876712328767123,\n",
       " 0.41095890410958896,\n",
       " 0.4520547945205479,\n",
       " 0.4794520547945205,\n",
       " 0.3424657534246575,\n",
       " 0.4383561643835617,\n",
       " 0.2876712328767123,\n",
       " 1.3835616438356164,\n",
       " 0.0547945205479452,\n",
       " 0.1232876712328767,\n",
       " 0.3972602739726027,\n",
       " 0.2602739726027397,\n",
       " 0.1643835616438356,\n",
       " 0.0958904109589041,\n",
       " 0.2602739726027397,\n",
       " 0.2191780821917808,\n",
       " 0.0958904109589041,\n",
       " 0.1506849315068493,\n",
       " 0.3698630136986301,\n",
       " 0.35616438356164376,\n",
       " 0.1643835616438356,\n",
       " 0.2465753424657534,\n",
       " 0.3972602739726027,\n",
       " 0.5068493150684932,\n",
       " 0.1232876712328767,\n",
       " 0.27397260273972607,\n",
       " 0.0547945205479452,\n",
       " 0.35616438356164376,\n",
       " 0.4383561643835617,\n",
       " 0.2191780821917808,\n",
       " 0.4931506849315069,\n",
       " 0.6164383561643836,\n",
       " 0.35616438356164376,\n",
       " 0.6849315068493149,\n",
       " 0.1780821917808219,\n",
       " 0.32876712328767127,\n",
       " 0.30136986301369856,\n",
       " 0.1780821917808219,\n",
       " 0.46575342465753417,\n",
       " 0.6027397260273972,\n",
       " 0.27397260273972607,\n",
       " 0.30136986301369856,\n",
       " 0.5068493150684932,\n",
       " 0.1506849315068493,\n",
       " 0.3150684931506849,\n",
       " 0.3698630136986301,\n",
       " 0.6301369863013697,\n",
       " 0.0821917808219178,\n",
       " 0.41095890410958896,\n",
       " 0.0684931506849315,\n",
       " 0.7671232876712328,\n",
       " 0.0,\n",
       " 0.1095890410958904,\n",
       " 0.2876712328767123,\n",
       " 0.0547945205479452,\n",
       " 0.136986301369863,\n",
       " 0.35616438356164376,\n",
       " 0.2328767123287671,\n",
       " 0.5205479452054793,\n",
       " 0.1643835616438356,\n",
       " 0.547945205479452,\n",
       " 0.547945205479452,\n",
       " 0.4383561643835617,\n",
       " 0.1780821917808219,\n",
       " 0.35616438356164376,\n",
       " 0.46575342465753417,\n",
       " 0.0410958904109589,\n",
       " 0.4794520547945205,\n",
       " 0.4383561643835617,\n",
       " 0.35616438356164376,\n",
       " 0.1643835616438356,\n",
       " 0.3150684931506849,\n",
       " 0.3972602739726027,\n",
       " 0.32876712328767127,\n",
       " 0.1780821917808219,\n",
       " 0.1506849315068493,\n",
       " 0.4794520547945205,\n",
       " 0.2602739726027397,\n",
       " 0.0,\n",
       " 0.3150684931506849,\n",
       " 0.3150684931506849,\n",
       " 0.5616438356164384,\n",
       " 0.0273972602739726,\n",
       " 0.1232876712328767,\n",
       " 0.0547945205479452,\n",
       " 0.2191780821917808,\n",
       " 0.35616438356164376,\n",
       " 0.2191780821917808,\n",
       " 0.136986301369863,\n",
       " 0.2876712328767123,\n",
       " 0.2328767123287671,\n",
       " 0.2054794520547945,\n",
       " 0.547945205479452,\n",
       " 0.0684931506849315,\n",
       " 0.136986301369863,\n",
       " 0.4246575342465753,\n",
       " 0.0273972602739726,\n",
       " 0.2465753424657534,\n",
       " 0.3150684931506849,\n",
       " 0.1643835616438356,\n",
       " 0.4931506849315069,\n",
       " 0.0,\n",
       " 0.2465753424657534,\n",
       " 0.32876712328767127,\n",
       " 0.0684931506849315,\n",
       " 0.2602739726027397,\n",
       " 0.2191780821917808,\n",
       " 0.3698630136986301,\n",
       " 0.5616438356164384,\n",
       " 0.5205479452054793,\n",
       " 0.3150684931506849,\n",
       " 0.4520547945205479,\n",
       " 0.35616438356164376,\n",
       " 0.0410958904109589,\n",
       " 0.30136986301369856,\n",
       " 0.5205479452054793,\n",
       " 0.4383561643835617,\n",
       " 0.5205479452054793,\n",
       " 0.547945205479452,\n",
       " 0.2191780821917808,\n",
       " 0.0136986301369863,\n",
       " 0.3835616438356165,\n",
       " 0.2191780821917808,\n",
       " 0.1780821917808219,\n",
       " 0.1095890410958904,\n",
       " 0.2191780821917808,\n",
       " 0.0273972602739726,\n",
       " 1.0,\n",
       " 0.6027397260273972,\n",
       " 0.5342465753424657,\n",
       " 0.5342465753424657,\n",
       " 0.5205479452054793,\n",
       " 0.0958904109589041,\n",
       " 0.3972602739726027,\n",
       " 0.0273972602739726,\n",
       " 0.35616438356164376,\n",
       " 0.2191780821917808,\n",
       " 0.41095890410958896,\n",
       " 0.4794520547945205,\n",
       " 1.1095890410958904,\n",
       " 0.35616438356164376,\n",
       " 0.8630136986301369,\n",
       " 0.3972602739726027,\n",
       " 0.1780821917808219,\n",
       " 0.1780821917808219,\n",
       " 1.4246575342465753,\n",
       " 0.136986301369863,\n",
       " 0.7808219178082192,\n",
       " 0.6027397260273972,\n",
       " 0.0,\n",
       " 0.0958904109589041,\n",
       " 0.41095890410958896,\n",
       " 0.3698630136986301,\n",
       " 0.32876712328767127,\n",
       " 0.0547945205479452,\n",
       " 0.32876712328767127,\n",
       " 0.5205479452054793,\n",
       " 0.1506849315068493,\n",
       " 0.5068493150684932,\n",
       " 0.1643835616438356,\n",
       " 0.5205479452054793,\n",
       " 0.0958904109589041,\n",
       " 0.2054794520547945,\n",
       " 0.6164383561643836,\n",
       " 0.2465753424657534,\n",
       " 0.0684931506849315,\n",
       " 0.41095890410958896,\n",
       " 0.3972602739726027,\n",
       " 0.2054794520547945,\n",
       " 0.3150684931506849,\n",
       " 0.0684931506849315,\n",
       " 0.5753424657534245,\n",
       " 0.4246575342465753,\n",
       " 0.547945205479452,\n",
       " 0.1917808219178082,\n",
       " 0.0273972602739726,\n",
       " 0.1506849315068493,\n",
       " 0.4246575342465753,\n",
       " 0.1780821917808219,\n",
       " 0.2054794520547945,\n",
       " 0.0821917808219178,\n",
       " 0.2328767123287671,\n",
       " 0.0136986301369863,\n",
       " 0.2054794520547945,\n",
       " 0.2191780821917808,\n",
       " 0.0273972602739726,\n",
       " 0.0547945205479452,\n",
       " 0.3972602739726027,\n",
       " 1.2602739726027397,\n",
       " 0.0958904109589041,\n",
       " 0.0410958904109589,\n",
       " 0.4520547945205479,\n",
       " 0.1095890410958904,\n",
       " 0.4794520547945205,\n",
       " 0.2191780821917808,\n",
       " 0.1095890410958904,\n",
       " 0.0547945205479452,\n",
       " 0.3150684931506849,\n",
       " 0.1780821917808219,\n",
       " 0.3150684931506849,\n",
       " 0.32876712328767127,\n",
       " 0.2054794520547945,\n",
       " 0.0958904109589041,\n",
       " 0.136986301369863,\n",
       " 0.4931506849315069,\n",
       " 0.0821917808219178,\n",
       " 0.2465753424657534,\n",
       " 0.0547945205479452,\n",
       " 0.35616438356164376,\n",
       " 0.3424657534246575,\n",
       " 0.2876712328767123,\n",
       " 0.30136986301369856,\n",
       " 0.27397260273972607,\n",
       " 0.2876712328767123,\n",
       " 0.35616438356164376,\n",
       " 0.3424657534246575,\n",
       " 0.1506849315068493,\n",
       " 0.0684931506849315,\n",
       " 0.46575342465753417,\n",
       " 0.1917808219178082,\n",
       " 0.5205479452054793,\n",
       " 1.2876712328767124,\n",
       " 0.6575342465753424,\n",
       " 1.2876712328767124,\n",
       " 0.0821917808219178,\n",
       " 0.0821917808219178,\n",
       " 0.6849315068493149,\n",
       " 0.1095890410958904,\n",
       " 0.1506849315068493,\n",
       " 0.2191780821917808,\n",
       " 0.2876712328767123,\n",
       " 0.4794520547945205,\n",
       " 0.5205479452054793,\n",
       " 0.4520547945205479,\n",
       " 0.3698630136986301,\n",
       " 0.4246575342465753,\n",
       " 0.4931506849315069,\n",
       " 0.0958904109589041,\n",
       " 0.0273972602739726,\n",
       " 0.2876712328767123,\n",
       " 0.27397260273972607,\n",
       " 0.6712328767123288,\n",
       " 0.0821917808219178,\n",
       " 0.2876712328767123,\n",
       " 0.3150684931506849,\n",
       " 0.1095890410958904,\n",
       " 0.0136986301369863,\n",
       " 0.1506849315068493,\n",
       " 0.2465753424657534,\n",
       " 0.2876712328767123,\n",
       " 0.5068493150684932,\n",
       " 0.2602739726027397,\n",
       " 0.1780821917808219,\n",
       " 0.30136986301369856,\n",
       " 0.2465753424657534,\n",
       " 0.3835616438356165,\n",
       " 0.3835616438356165,\n",
       " 0.0273972602739726,\n",
       " 0.1643835616438356,\n",
       " 0.5753424657534245,\n",
       " 0.1095890410958904,\n",
       " 0.0547945205479452,\n",
       " 0.0547945205479452,\n",
       " 0.32876712328767127,\n",
       " 0.0684931506849315,\n",
       " 0.2876712328767123,\n",
       " 0.41095890410958896,\n",
       " 0.1780821917808219,\n",
       " 0.35616438356164376,\n",
       " 0.1917808219178082,\n",
       " 0.0136986301369863,\n",
       " 0.1917808219178082,\n",
       " 0.1780821917808219,\n",
       " 0.2191780821917808,\n",
       " 1.410958904109589,\n",
       " 0.2876712328767123,\n",
       " 0.27397260273972607,\n",
       " 0.3835616438356165,\n",
       " 0.0821917808219178,\n",
       " 0.2602739726027397,\n",
       " 0.2054794520547945,\n",
       " 0.46575342465753417,\n",
       " 0.2876712328767123,\n",
       " 0.7123287671232876,\n",
       " 0.1232876712328767,\n",
       " 0.5890410958904109,\n",
       " 0.726027397260274,\n",
       " 0.2876712328767123,\n",
       " 0.46575342465753417,\n",
       " 1.095890410958904,\n",
       " 0.32876712328767127,\n",
       " 0.6849315068493149,\n",
       " 0.1780821917808219,\n",
       " 0.4931506849315069,\n",
       " 0.41095890410958896,\n",
       " 0.1917808219178082,\n",
       " 0.1095890410958904,\n",
       " 0.0821917808219178,\n",
       " 0.1917808219178082,\n",
       " 0.3424657534246575,\n",
       " 0.32876712328767127,\n",
       " 0.35616438356164376,\n",
       " 0.2054794520547945,\n",
       " 0.0136986301369863,\n",
       " 0.1643835616438356,\n",
       " 1.3150684931506849,\n",
       " 0.32876712328767127,\n",
       " 0.821917808219178,\n",
       " 0.0547945205479452,\n",
       " 0.0547945205479452,\n",
       " 0.0684931506849315,\n",
       " 0.0958904109589041,\n",
       " 0.3150684931506849,\n",
       " 0.0684931506849315,\n",
       " 0.4383561643835617,\n",
       " 0.2191780821917808,\n",
       " 0.0410958904109589,\n",
       " 0.4931506849315069,\n",
       " 0.1095890410958904,\n",
       " 0.3835616438356165,\n",
       " 0.5342465753424657,\n",
       " 0.41095890410958896,\n",
       " 0.0547945205479452,\n",
       " 0.1095890410958904,\n",
       " 0.2191780821917808,\n",
       " 0.6301369863013697,\n",
       " 0.32876712328767127,\n",
       " 1.3287671232876712,\n",
       " 1.4931506849315068,\n",
       " 0.6027397260273972,\n",
       " 0.3972602739726027,\n",
       " 0.5205479452054793,\n",
       " 1.1232876712328768,\n",
       " 0.0136986301369863,\n",
       " 0.136986301369863,\n",
       " 0.1095890410958904,\n",
       " 0.0958904109589041,\n",
       " 0.5616438356164384,\n",
       " 0.136986301369863,\n",
       " 0.0410958904109589,\n",
       " 0.3424657534246575,\n",
       " 0.4520547945205479,\n",
       " 0.6164383561643836,\n",
       " 0.2602739726027397,\n",
       " 0.2602739726027397,\n",
       " 0.2191780821917808,\n",
       " 1.6575342465753424,\n",
       " 0.136986301369863,\n",
       " 0.0136986301369863,\n",
       " 0.6301369863013697,\n",
       " 0.6575342465753424,\n",
       " 1.547945205479452,\n",
       " 0.1780821917808219,\n",
       " ...]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ = df.rdd.mapPartitions(test).collect()\n",
    "test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "86b31912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04881441965699196"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(fm(x_test.head(1).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fe3cfcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-05 20:48:08.836404: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-05 20:48:13.844180: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/05 20:48:14 ERROR Executor: Exception in task 0.0 in stage 43.0 (TID 43)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1884, in __getitem__\n",
      "    idx = self.__fields__.index(item)\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n",
      "    return f(iterator)\n",
      "  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/226376529.py\", line 17, in predict\n",
      "  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/226376529.py\", line 17, in <listcomp>\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1889, in __getitem__\n",
      "    raise ValueError(item)\n",
      "ValueError: Index(['age', 'workclass- ?', 'workclass- Federal-gov', 'workclass- Local-gov',\n",
      "       'workclass- Never-worked', 'workclass- Private',\n",
      "       'workclass- Self-emp-inc', 'workclass- Self-emp-not-inc',\n",
      "       'workclass- State-gov', 'workclass- Without-pay',\n",
      "       ...\n",
      "       'country- Portugal', 'country- Puerto-Rico', 'country- Scotland',\n",
      "       'country- South', 'country- Taiwan', 'country- Thailand',\n",
      "       'country- Trinadad&Tobago', 'country- United-States',\n",
      "       'country- Vietnam', 'country- Yugoslavia'],\n",
      "      dtype='object', length=108)\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:831)\n",
      "23/02/05 20:48:14 WARN TaskSetManager: Lost task 0.0 in stage 43.0 (TID 43) (192.168.0.24 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1884, in __getitem__\n",
      "    idx = self.__fields__.index(item)\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n",
      "    return f(iterator)\n",
      "  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/226376529.py\", line 17, in predict\n",
      "  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/226376529.py\", line 17, in <listcomp>\n",
      "  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1889, in __getitem__\n",
      "    raise ValueError(item)\n",
      "ValueError: Index(['age', 'workclass- ?', 'workclass- Federal-gov', 'workclass- Local-gov',\n",
      "       'workclass- Never-worked', 'workclass- Private',\n",
      "       'workclass- Self-emp-inc', 'workclass- Self-emp-not-inc',\n",
      "       'workclass- State-gov', 'workclass- Without-pay',\n",
      "       ...\n",
      "       'country- Portugal', 'country- Puerto-Rico', 'country- Scotland',\n",
      "       'country- South', 'country- Taiwan', 'country- Thailand',\n",
      "       'country- Trinadad&Tobago', 'country- United-States',\n",
      "       'country- Vietnam', 'country- Yugoslavia'],\n",
      "      dtype='object', length=108)\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:831)\n",
      "\n",
      "23/02/05 20:48:14 ERROR TaskSetManager: Task 0 in stage 43.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 1 times, most recent failure: Lost task 0.0 in stage 43.0 (TID 43) (192.168.0.24 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1884, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/226376529.py\", line 17, in predict\n  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/226376529.py\", line 17, in <listcomp>\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1889, in __getitem__\n    raise ValueError(item)\nValueError: Index(['age', 'workclass- ?', 'workclass- Federal-gov', 'workclass- Local-gov',\n       'workclass- Never-worked', 'workclass- Private',\n       'workclass- Self-emp-inc', 'workclass- Self-emp-not-inc',\n       'workclass- State-gov', 'workclass- Without-pay',\n       ...\n       'country- Portugal', 'country- Puerto-Rico', 'country- Scotland',\n       'country- South', 'country- Taiwan', 'country- Thailand',\n       'country- Trinadad&Tobago', 'country- United-States',\n       'country- Vietnam', 'country- Yugoslavia'],\n      dtype='object', length=108)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:831)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:831)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1884, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/226376529.py\", line 17, in predict\n  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/226376529.py\", line 17, in <listcomp>\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1889, in __getitem__\n    raise ValueError(item)\nValueError: Index(['age', 'workclass- ?', 'workclass- Federal-gov', 'workclass- Local-gov',\n       'workclass- Never-worked', 'workclass- Private',\n       'workclass- Self-emp-inc', 'workclass- Self-emp-not-inc',\n       'workclass- State-gov', 'workclass- Without-pay',\n       ...\n       'country- Portugal', 'country- Puerto-Rico', 'country- Scotland',\n       'country- South', 'country- Taiwan', 'country- Thailand',\n       'country- Trinadad&Tobago', 'country- United-States',\n       'country- Vietnam', 'country- Yugoslavia'],\n      dtype='object', length=108)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Use the mapPartitions function to make predictions on the data\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m predictions\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1197\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 1 times, most recent failure: Lost task 0.0 in stage 43.0 (TID 43) (192.168.0.24 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1884, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/226376529.py\", line 17, in predict\n  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/226376529.py\", line 17, in <listcomp>\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1889, in __getitem__\n    raise ValueError(item)\nValueError: Index(['age', 'workclass- ?', 'workclass- Federal-gov', 'workclass- Local-gov',\n       'workclass- Never-worked', 'workclass- Private',\n       'workclass- Self-emp-inc', 'workclass- Self-emp-not-inc',\n       'workclass- State-gov', 'workclass- Without-pay',\n       ...\n       'country- Portugal', 'country- Puerto-Rico', 'country- Scotland',\n       'country- South', 'country- Taiwan', 'country- Thailand',\n       'country- Trinadad&Tobago', 'country- United-States',\n       'country- Vietnam', 'country- Yugoslavia'],\n      dtype='object', length=108)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:831)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:831)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1884, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/226376529.py\", line 17, in predict\n  File \"/var/folders/y7/ctbm4_yn3_11qs65_7zhkdcm0000gn/T/ipykernel_9775/226376529.py\", line 17, in <listcomp>\n  File \"/Users/youngyong/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1889, in __getitem__\n    raise ValueError(item)\nValueError: Index(['age', 'workclass- ?', 'workclass- Federal-gov', 'workclass- Local-gov',\n       'workclass- Never-worked', 'workclass- Private',\n       'workclass- Self-emp-inc', 'workclass- Self-emp-not-inc',\n       'workclass- State-gov', 'workclass- Without-pay',\n       ...\n       'country- Portugal', 'country- Puerto-Rico', 'country- Scotland',\n       'country- South', 'country- Taiwan', 'country- Thailand',\n       'country- Trinadad&Tobago', 'country- United-States',\n       'country- Vietnam', 'country- Yugoslavia'],\n      dtype='object', length=108)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "FEATURES = x_test.columns\n",
    "path = './weights/weights-epoch(10)-batch(256)-embedding(5).h5' \n",
    "\n",
    "# Load the data into a PySpark DataFrame\n",
    "df = spark.read.parquet('data/test.parquet', header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# Define the function that will make predictions with the model\n",
    "def predict(iterator):\n",
    "    # model load\n",
    "    fm = DeepFM(embedding_size=config.EMBEDDING_SIZE, num_feature=len(field_index),\n",
    "               num_field=len(field_dict), field_index=field_index)\n",
    "    fm.build(input_shape = (1,len(field_index)))\n",
    "    fm.load_weights(path)\n",
    "\n",
    "    # Make predictions for each row in the iterator\n",
    "    results = [float(fm(row[FEATURES].values)) for row in iterator]\n",
    "\n",
    "    # Return the results\n",
    "    return results\n",
    "\n",
    "# Use the mapPartitions function to make predictions on the data\n",
    "predictions = df.rdd.mapPartitions(predict).collect()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40315a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee244890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdab648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9a8f162",
   "metadata": {},
   "source": [
    "### MapPartitions example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ed0b700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n",
      "|firstname|lastname|gender|salary|\n",
      "+---------+--------+------+------+\n",
      "|    James|   Smith|     M|  3000|\n",
      "|     Anna|    Rose|     F|  4100|\n",
      "|   Robert|Williams|     M|  6200|\n",
      "+---------+--------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data = [('James','Smith','M',3000),\n",
    "  ('Anna','Rose','F',4100),\n",
    "  ('Robert','Williams','M',6200), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1c0a83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           name|bonus|\n",
      "+---------------+-----+\n",
      "|    James,Smith|300.0|\n",
      "|      Anna,Rose|410.0|\n",
      "|Robert,Williams|620.0|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This function calls for each partition\n",
    "def reformat(partitionData):\n",
    "    for row in partitionData:\n",
    "        yield [row['firstname']+\",\"+row.lastname,row.salary*10/100]\n",
    "\n",
    "df2=df.rdd.mapPartitions(reformat).toDF([\"name\",\"bonus\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e94e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
